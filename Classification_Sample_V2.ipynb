{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter searching: https://www.projectpro.io/recipes/find-optimal-parameters-using-gridsearchcv \\\n",
    "Renaming the last column: https://stackoverflow.com/questions/56479835/rename-only-the-last-column-in-pandas-dataframe-accounting-for-duplicate-header"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in dataset normal, STM and BerTopic\n",
    "df = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Sample\\\\sample.csv\")\n",
    "df_STM = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Sample\\\\features_sample_STM.csv\")\n",
    "df_Bert = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Sample\\\\features_sample_Bert_reduced.csv\")\n",
    "df_STM_selected = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Sample\\\\Sample_featured_selected_STM.csv\")\n",
    "df_Bert_selected = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Sample\\\\Sample_featured_selected_Bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_STM.columns = [*df_STM.columns[:-1], 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V112</th>\n",
       "      <th>V113</th>\n",
       "      <th>V114</th>\n",
       "      <th>V115</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V118</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.039118</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.052227</td>\n",
       "      <td>0.007085</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.030866</td>\n",
       "      <td>0.010538</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210494</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.045555</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020091</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.017580</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.054394</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.039413</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25884</th>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.108549</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.057887</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>0.040965</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.063389</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25885</th>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>0.074855</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.046063</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25886</th>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.021911</td>\n",
       "      <td>0.061482</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25887</th>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.045607</td>\n",
       "      <td>0.017417</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25888</th>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.015891</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.067107</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>0.013039</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.025677</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25889 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      0.001030  0.002027  0.000527  0.000378  0.001124  0.039118  0.012707   \n",
       "1      0.000851  0.001892  0.000647  0.000380  0.001040  0.029230  0.006719   \n",
       "2      0.000708  0.001753  0.000584  0.000282  0.000680  0.030866  0.010538   \n",
       "3      0.001218  0.005576  0.012048  0.008318  0.001654  0.045555  0.005865   \n",
       "4      0.001521  0.003697  0.000844  0.000252  0.001784  0.039413  0.005086   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "25884  0.001353  0.108549  0.000977  0.000345  0.003568  0.057887  0.007655   \n",
       "25885  0.001054  0.007069  0.074855  0.000221  0.001314  0.032114  0.008504   \n",
       "25886  0.000753  0.001885  0.001025  0.000213  0.001507  0.019102  0.005220   \n",
       "25887  0.002256  0.002623  0.000846  0.000277  0.002363  0.045607  0.017417   \n",
       "25888  0.001813  0.005340  0.015891  0.000394  0.001957  0.067107  0.013095   \n",
       "\n",
       "             V8        V9       V10  ...      V112      V113      V114  \\\n",
       "0      0.006189  0.003038  0.001320  ...  0.004531  0.002872  0.001559   \n",
       "1      0.052227  0.007085  0.001319  ...  0.014664  0.004294  0.001827   \n",
       "2      0.006753  0.017031  0.001409  ...  0.210494  0.019108  0.001475   \n",
       "3      0.007628  0.003562  0.001704  ...  0.020091  0.002962  0.000892   \n",
       "4      0.021152  0.013926  0.000902  ...  0.009288  0.000816  0.002093   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "25884  0.040965  0.016742  0.000837  ...  0.003300  0.001532  0.001557   \n",
       "25885  0.043870  0.004286  0.000655  ...  0.001840  0.000963  0.000830   \n",
       "25886  0.021911  0.061482  0.000588  ...  0.012958  0.000560  0.001571   \n",
       "25887  0.012401  0.004876  0.000957  ...  0.022726  0.001375  0.002251   \n",
       "25888  0.013039  0.007810  0.000902  ...  0.004087  0.011824  0.002695   \n",
       "\n",
       "           V115      V116      V117      V118      V119      V120  sentiment  \n",
       "0      0.000962  0.014338  0.001158  0.008602  0.010092  0.002836          0  \n",
       "1      0.000721  0.002362  0.001433  0.004187  0.007426  0.003151          1  \n",
       "2      0.000930  0.001968  0.000651  0.005079  0.021736  0.002512          0  \n",
       "3      0.001102  0.017580  0.001325  0.008031  0.054394  0.003069          0  \n",
       "4      0.000774  0.001825  0.002602  0.002455  0.005683  0.002809          1  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "25884  0.002331  0.002155  0.004349  0.002952  0.063389  0.003008          0  \n",
       "25885  0.032408  0.046063  0.001052  0.005064  0.004210  0.002734          1  \n",
       "25886  0.000825  0.001197  0.002342  0.001518  0.005652  0.002274          1  \n",
       "25887  0.013894  0.025253  0.000847  0.002625  0.002722  0.002950          0  \n",
       "25888  0.018175  0.002415  0.003590  0.005979  0.025677  0.003337          0  \n",
       "\n",
       "[25889 rows x 121 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_STM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.406603e-02</td>\n",
       "      <td>1.631666e-01</td>\n",
       "      <td>2.190369e-02</td>\n",
       "      <td>6.570173e-03</td>\n",
       "      <td>2.307655e-02</td>\n",
       "      <td>8.365412e-03</td>\n",
       "      <td>1.044651e-03</td>\n",
       "      <td>1.436761e-02</td>\n",
       "      <td>7.665692e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.605185e-03</td>\n",
       "      <td>8.505731e-03</td>\n",
       "      <td>4.350101e-04</td>\n",
       "      <td>1.077927e-03</td>\n",
       "      <td>1.680891e-03</td>\n",
       "      <td>1.544089e-03</td>\n",
       "      <td>1.067855e-03</td>\n",
       "      <td>1.870928e-03</td>\n",
       "      <td>1.215840e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.937321e-306</td>\n",
       "      <td>9.402316e-306</td>\n",
       "      <td>5.238678e-306</td>\n",
       "      <td>2.618513e-306</td>\n",
       "      <td>2.077104e-305</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.243770e-307</td>\n",
       "      <td>6.344945e-306</td>\n",
       "      <td>1.919045e-306</td>\n",
       "      <td>...</td>\n",
       "      <td>3.224686e-307</td>\n",
       "      <td>3.406447e-307</td>\n",
       "      <td>2.460309e-307</td>\n",
       "      <td>5.507471e-307</td>\n",
       "      <td>1.134362e-306</td>\n",
       "      <td>4.168658e-306</td>\n",
       "      <td>1.944866e-307</td>\n",
       "      <td>5.278984e-307</td>\n",
       "      <td>1.474514e-306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.609316e-02</td>\n",
       "      <td>3.192536e-02</td>\n",
       "      <td>2.046305e-02</td>\n",
       "      <td>1.055913e-02</td>\n",
       "      <td>5.270181e-02</td>\n",
       "      <td>8.974301e-02</td>\n",
       "      <td>2.049910e-03</td>\n",
       "      <td>1.965568e-02</td>\n",
       "      <td>6.258281e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221447e-03</td>\n",
       "      <td>1.183277e-03</td>\n",
       "      <td>7.689357e-04</td>\n",
       "      <td>1.690983e-03</td>\n",
       "      <td>5.492027e-03</td>\n",
       "      <td>6.932800e-03</td>\n",
       "      <td>5.895996e-04</td>\n",
       "      <td>1.672702e-03</td>\n",
       "      <td>3.622286e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.426547e-02</td>\n",
       "      <td>1.562201e-01</td>\n",
       "      <td>2.260713e-02</td>\n",
       "      <td>6.744916e-03</td>\n",
       "      <td>2.336913e-02</td>\n",
       "      <td>8.507393e-03</td>\n",
       "      <td>1.069602e-03</td>\n",
       "      <td>1.459541e-02</td>\n",
       "      <td>7.799090e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.664065e-03</td>\n",
       "      <td>9.642235e-03</td>\n",
       "      <td>4.452774e-04</td>\n",
       "      <td>1.096405e-03</td>\n",
       "      <td>1.713376e-03</td>\n",
       "      <td>1.565868e-03</td>\n",
       "      <td>1.098450e-03</td>\n",
       "      <td>1.889294e-03</td>\n",
       "      <td>1.235315e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6.190984e-306</td>\n",
       "      <td>1.572890e-305</td>\n",
       "      <td>7.223648e-306</td>\n",
       "      <td>3.001427e-306</td>\n",
       "      <td>1.666965e-305</td>\n",
       "      <td>8.542622e-306</td>\n",
       "      <td>5.309188e-307</td>\n",
       "      <td>6.200085e-306</td>\n",
       "      <td>2.160959e-306</td>\n",
       "      <td>...</td>\n",
       "      <td>4.320584e-307</td>\n",
       "      <td>4.865545e-307</td>\n",
       "      <td>2.079111e-307</td>\n",
       "      <td>4.988167e-307</td>\n",
       "      <td>1.660531e-306</td>\n",
       "      <td>1.586303e-306</td>\n",
       "      <td>2.187228e-307</td>\n",
       "      <td>6.287674e-307</td>\n",
       "      <td>8.428811e-307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25896</th>\n",
       "      <td>25896</td>\n",
       "      <td>1.320379e-02</td>\n",
       "      <td>3.576054e-02</td>\n",
       "      <td>3.501184e-02</td>\n",
       "      <td>1.040078e-02</td>\n",
       "      <td>2.400560e-02</td>\n",
       "      <td>1.242579e-02</td>\n",
       "      <td>1.390931e-03</td>\n",
       "      <td>1.207024e-02</td>\n",
       "      <td>4.813511e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.777248e-03</td>\n",
       "      <td>1.608226e-03</td>\n",
       "      <td>5.245896e-04</td>\n",
       "      <td>9.712274e-04</td>\n",
       "      <td>4.733079e-03</td>\n",
       "      <td>1.968536e-03</td>\n",
       "      <td>5.950869e-04</td>\n",
       "      <td>1.239214e-03</td>\n",
       "      <td>1.355423e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25897</th>\n",
       "      <td>25897</td>\n",
       "      <td>4.529771e-03</td>\n",
       "      <td>1.179237e-02</td>\n",
       "      <td>7.929149e-03</td>\n",
       "      <td>2.164511e-03</td>\n",
       "      <td>5.079875e-03</td>\n",
       "      <td>2.026990e-03</td>\n",
       "      <td>4.415183e-04</td>\n",
       "      <td>3.359419e-03</td>\n",
       "      <td>1.820484e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.053673e-04</td>\n",
       "      <td>1.414617e-03</td>\n",
       "      <td>1.840088e-04</td>\n",
       "      <td>2.616364e-04</td>\n",
       "      <td>4.209857e-04</td>\n",
       "      <td>3.494251e-04</td>\n",
       "      <td>4.217197e-04</td>\n",
       "      <td>3.731257e-04</td>\n",
       "      <td>3.070442e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25898</th>\n",
       "      <td>25898</td>\n",
       "      <td>1.553569e-02</td>\n",
       "      <td>3.046587e-02</td>\n",
       "      <td>2.226832e-02</td>\n",
       "      <td>1.255153e-02</td>\n",
       "      <td>4.626328e-02</td>\n",
       "      <td>2.650033e-01</td>\n",
       "      <td>2.475263e-03</td>\n",
       "      <td>1.914384e-02</td>\n",
       "      <td>6.250478e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313843e-03</td>\n",
       "      <td>1.202768e-03</td>\n",
       "      <td>8.912865e-04</td>\n",
       "      <td>1.679637e-03</td>\n",
       "      <td>6.112104e-03</td>\n",
       "      <td>5.255948e-03</td>\n",
       "      <td>6.175675e-04</td>\n",
       "      <td>1.593530e-03</td>\n",
       "      <td>3.395612e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25899</th>\n",
       "      <td>25899</td>\n",
       "      <td>4.562475e-03</td>\n",
       "      <td>1.180991e-02</td>\n",
       "      <td>7.923741e-03</td>\n",
       "      <td>2.173168e-03</td>\n",
       "      <td>5.106683e-03</td>\n",
       "      <td>2.035990e-03</td>\n",
       "      <td>4.444279e-04</td>\n",
       "      <td>3.383648e-03</td>\n",
       "      <td>1.836676e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.050692e-04</td>\n",
       "      <td>1.407580e-03</td>\n",
       "      <td>1.853717e-04</td>\n",
       "      <td>2.634941e-04</td>\n",
       "      <td>4.219986e-04</td>\n",
       "      <td>3.510347e-04</td>\n",
       "      <td>4.273203e-04</td>\n",
       "      <td>3.756459e-04</td>\n",
       "      <td>3.087271e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25900</th>\n",
       "      <td>25900</td>\n",
       "      <td>4.525197e-03</td>\n",
       "      <td>1.177046e-02</td>\n",
       "      <td>7.946571e-03</td>\n",
       "      <td>2.167568e-03</td>\n",
       "      <td>5.079315e-03</td>\n",
       "      <td>2.027654e-03</td>\n",
       "      <td>4.418974e-04</td>\n",
       "      <td>3.358756e-03</td>\n",
       "      <td>1.819407e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.075501e-04</td>\n",
       "      <td>1.407354e-03</td>\n",
       "      <td>1.841403e-04</td>\n",
       "      <td>2.616282e-04</td>\n",
       "      <td>4.212455e-04</td>\n",
       "      <td>3.494509e-04</td>\n",
       "      <td>4.211917e-04</td>\n",
       "      <td>3.728618e-04</td>\n",
       "      <td>3.070852e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25901 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0              0              1              2              3  \\\n",
       "0               0   2.406603e-02   1.631666e-01   2.190369e-02   6.570173e-03   \n",
       "1               1  4.937321e-306  9.402316e-306  5.238678e-306  2.618513e-306   \n",
       "2               2   1.609316e-02   3.192536e-02   2.046305e-02   1.055913e-02   \n",
       "3               3   2.426547e-02   1.562201e-01   2.260713e-02   6.744916e-03   \n",
       "4               4  6.190984e-306  1.572890e-305  7.223648e-306  3.001427e-306   \n",
       "...           ...            ...            ...            ...            ...   \n",
       "25896       25896   1.320379e-02   3.576054e-02   3.501184e-02   1.040078e-02   \n",
       "25897       25897   4.529771e-03   1.179237e-02   7.929149e-03   2.164511e-03   \n",
       "25898       25898   1.553569e-02   3.046587e-02   2.226832e-02   1.255153e-02   \n",
       "25899       25899   4.562475e-03   1.180991e-02   7.923741e-03   2.173168e-03   \n",
       "25900       25900   4.525197e-03   1.177046e-02   7.946571e-03   2.167568e-03   \n",
       "\n",
       "                   4              5              6              7  \\\n",
       "0       2.307655e-02   8.365412e-03   1.044651e-03   1.436761e-02   \n",
       "1      2.077104e-305   1.000000e+00  6.243770e-307  6.344945e-306   \n",
       "2       5.270181e-02   8.974301e-02   2.049910e-03   1.965568e-02   \n",
       "3       2.336913e-02   8.507393e-03   1.069602e-03   1.459541e-02   \n",
       "4      1.666965e-305  8.542622e-306  5.309188e-307  6.200085e-306   \n",
       "...              ...            ...            ...            ...   \n",
       "25896   2.400560e-02   1.242579e-02   1.390931e-03   1.207024e-02   \n",
       "25897   5.079875e-03   2.026990e-03   4.415183e-04   3.359419e-03   \n",
       "25898   4.626328e-02   2.650033e-01   2.475263e-03   1.914384e-02   \n",
       "25899   5.106683e-03   2.035990e-03   4.444279e-04   3.383648e-03   \n",
       "25900   5.079315e-03   2.027654e-03   4.418974e-04   3.358756e-03   \n",
       "\n",
       "                   8  ...            111            112            113  \\\n",
       "0       7.665692e-03  ...   1.605185e-03   8.505731e-03   4.350101e-04   \n",
       "1      1.919045e-306  ...  3.224686e-307  3.406447e-307  2.460309e-307   \n",
       "2       6.258281e-03  ...   1.221447e-03   1.183277e-03   7.689357e-04   \n",
       "3       7.799090e-03  ...   1.664065e-03   9.642235e-03   4.452774e-04   \n",
       "4      2.160959e-306  ...  4.320584e-307  4.865545e-307  2.079111e-307   \n",
       "...              ...  ...            ...            ...            ...   \n",
       "25896   4.813511e-03  ...   1.777248e-03   1.608226e-03   5.245896e-04   \n",
       "25897   1.820484e-03  ...   7.053673e-04   1.414617e-03   1.840088e-04   \n",
       "25898   6.250478e-03  ...   1.313843e-03   1.202768e-03   8.912865e-04   \n",
       "25899   1.836676e-03  ...   7.050692e-04   1.407580e-03   1.853717e-04   \n",
       "25900   1.819407e-03  ...   7.075501e-04   1.407354e-03   1.841403e-04   \n",
       "\n",
       "                 114            115            116            117  \\\n",
       "0       1.077927e-03   1.680891e-03   1.544089e-03   1.067855e-03   \n",
       "1      5.507471e-307  1.134362e-306  4.168658e-306  1.944866e-307   \n",
       "2       1.690983e-03   5.492027e-03   6.932800e-03   5.895996e-04   \n",
       "3       1.096405e-03   1.713376e-03   1.565868e-03   1.098450e-03   \n",
       "4      4.988167e-307  1.660531e-306  1.586303e-306  2.187228e-307   \n",
       "...              ...            ...            ...            ...   \n",
       "25896   9.712274e-04   4.733079e-03   1.968536e-03   5.950869e-04   \n",
       "25897   2.616364e-04   4.209857e-04   3.494251e-04   4.217197e-04   \n",
       "25898   1.679637e-03   6.112104e-03   5.255948e-03   6.175675e-04   \n",
       "25899   2.634941e-04   4.219986e-04   3.510347e-04   4.273203e-04   \n",
       "25900   2.616282e-04   4.212455e-04   3.494509e-04   4.211917e-04   \n",
       "\n",
       "                 118            119  sentiment  \n",
       "0       1.870928e-03   1.215840e-03          0  \n",
       "1      5.278984e-307  1.474514e-306          1  \n",
       "2       1.672702e-03   3.622286e-03          0  \n",
       "3       1.889294e-03   1.235315e-03          0  \n",
       "4      6.287674e-307  8.428811e-307          1  \n",
       "...              ...            ...        ...  \n",
       "25896   1.239214e-03   1.355423e-03          0  \n",
       "25897   3.731257e-04   3.070442e-04          1  \n",
       "25898   1.593530e-03   3.395612e-03          1  \n",
       "25899   3.756459e-04   3.087271e-04          0  \n",
       "25900   3.728618e-04   3.070852e-04          0  \n",
       "\n",
       "[25901 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>V111</th>\n",
       "      <th>V112</th>\n",
       "      <th>V113</th>\n",
       "      <th>V114</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V118</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.039118</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.052227</td>\n",
       "      <td>0.007085</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012356</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.030866</td>\n",
       "      <td>0.010538</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.210494</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>0.045555</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.020091</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.017580</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.054394</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.039413</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25884</th>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.108549</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.057887</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>0.040965</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014499</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.063389</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25885</th>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>0.074855</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.046063</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25886</th>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.021911</td>\n",
       "      <td>0.061482</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.024373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25887</th>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.045607</td>\n",
       "      <td>0.017417</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25888</th>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.015891</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.067107</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>0.013039</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.025677</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25889 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V6        V7        V8  \\\n",
       "0      0.001030  0.002027  0.000527  0.000378  0.039118  0.012707  0.006189   \n",
       "1      0.000851  0.001892  0.000647  0.000380  0.029230  0.006719  0.052227   \n",
       "2      0.000708  0.001753  0.000584  0.000282  0.030866  0.010538  0.006753   \n",
       "3      0.001218  0.005576  0.012048  0.008318  0.045555  0.005865  0.007628   \n",
       "4      0.001521  0.003697  0.000844  0.000252  0.039413  0.005086  0.021152   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "25884  0.001353  0.108549  0.000977  0.000345  0.057887  0.007655  0.040965   \n",
       "25885  0.001054  0.007069  0.074855  0.000221  0.032114  0.008504  0.043870   \n",
       "25886  0.000753  0.001885  0.001025  0.000213  0.019102  0.005220  0.021911   \n",
       "25887  0.002256  0.002623  0.000846  0.000277  0.045607  0.017417  0.012401   \n",
       "25888  0.001813  0.005340  0.015891  0.000394  0.067107  0.013095  0.013039   \n",
       "\n",
       "             V9       V10       V11  ...      V111      V112      V113  \\\n",
       "0      0.003038  0.001320  0.003013  ...  0.002468  0.004531  0.002872   \n",
       "1      0.007085  0.001319  0.001721  ...  0.012356  0.014664  0.004294   \n",
       "2      0.017031  0.001409  0.001778  ...  0.003094  0.210494  0.019108   \n",
       "3      0.003562  0.001704  0.001617  ...  0.002371  0.020091  0.002962   \n",
       "4      0.013926  0.000902  0.004319  ...  0.003226  0.009288  0.000816   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "25884  0.016742  0.000837  0.002989  ...  0.014499  0.003300  0.001532   \n",
       "25885  0.004286  0.000655  0.003588  ...  0.002434  0.001840  0.000963   \n",
       "25886  0.061482  0.000588  0.024373  ...  0.008500  0.012958  0.000560   \n",
       "25887  0.004876  0.000957  0.001413  ...  0.001832  0.022726  0.001375   \n",
       "25888  0.007810  0.000902  0.002266  ...  0.007301  0.004087  0.011824   \n",
       "\n",
       "           V114      V116      V117      V118      V119      V120  sentiment  \n",
       "0      0.001559  0.014338  0.001158  0.008602  0.010092  0.002836          0  \n",
       "1      0.001827  0.002362  0.001433  0.004187  0.007426  0.003151          1  \n",
       "2      0.001475  0.001968  0.000651  0.005079  0.021736  0.002512          0  \n",
       "3      0.000892  0.017580  0.001325  0.008031  0.054394  0.003069          0  \n",
       "4      0.002093  0.001825  0.002602  0.002455  0.005683  0.002809          1  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "25884  0.001557  0.002155  0.004349  0.002952  0.063389  0.003008          0  \n",
       "25885  0.000830  0.046063  0.001052  0.005064  0.004210  0.002734          1  \n",
       "25886  0.001571  0.001197  0.002342  0.001518  0.005652  0.002274          1  \n",
       "25887  0.002251  0.025253  0.000847  0.002625  0.002722  0.002950          0  \n",
       "25888  0.002695  0.002415  0.003590  0.005979  0.025677  0.003337          0  \n",
       "\n",
       "[25889 rows x 114 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_STM_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.406603e-02</td>\n",
       "      <td>1.631666e-01</td>\n",
       "      <td>2.307655e-02</td>\n",
       "      <td>8.365412e-03</td>\n",
       "      <td>1.044651e-03</td>\n",
       "      <td>1.436761e-02</td>\n",
       "      <td>7.665692e-03</td>\n",
       "      <td>1.521247e-02</td>\n",
       "      <td>2.381878e-02</td>\n",
       "      <td>4.134728e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>7.608225e-04</td>\n",
       "      <td>2.045898e-03</td>\n",
       "      <td>1.605185e-03</td>\n",
       "      <td>8.505731e-03</td>\n",
       "      <td>1.680891e-03</td>\n",
       "      <td>1.544089e-03</td>\n",
       "      <td>1.067855e-03</td>\n",
       "      <td>1.870928e-03</td>\n",
       "      <td>1.215840e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.937321e-306</td>\n",
       "      <td>9.402316e-306</td>\n",
       "      <td>2.077104e-305</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.243770e-307</td>\n",
       "      <td>6.344945e-306</td>\n",
       "      <td>1.919045e-306</td>\n",
       "      <td>9.030316e-306</td>\n",
       "      <td>1.043084e-305</td>\n",
       "      <td>8.912841e-306</td>\n",
       "      <td>...</td>\n",
       "      <td>3.389193e-307</td>\n",
       "      <td>9.537043e-307</td>\n",
       "      <td>3.224686e-307</td>\n",
       "      <td>3.406447e-307</td>\n",
       "      <td>1.134362e-306</td>\n",
       "      <td>4.168658e-306</td>\n",
       "      <td>1.944866e-307</td>\n",
       "      <td>5.278984e-307</td>\n",
       "      <td>1.474514e-306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.609316e-02</td>\n",
       "      <td>3.192536e-02</td>\n",
       "      <td>5.270181e-02</td>\n",
       "      <td>8.974301e-02</td>\n",
       "      <td>2.049910e-03</td>\n",
       "      <td>1.965568e-02</td>\n",
       "      <td>6.258281e-03</td>\n",
       "      <td>2.901898e-02</td>\n",
       "      <td>4.356667e-02</td>\n",
       "      <td>3.244086e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021346e-03</td>\n",
       "      <td>2.771639e-03</td>\n",
       "      <td>1.221447e-03</td>\n",
       "      <td>1.183277e-03</td>\n",
       "      <td>5.492027e-03</td>\n",
       "      <td>6.932800e-03</td>\n",
       "      <td>5.895996e-04</td>\n",
       "      <td>1.672702e-03</td>\n",
       "      <td>3.622286e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.426547e-02</td>\n",
       "      <td>1.562201e-01</td>\n",
       "      <td>2.336913e-02</td>\n",
       "      <td>8.507393e-03</td>\n",
       "      <td>1.069602e-03</td>\n",
       "      <td>1.459541e-02</td>\n",
       "      <td>7.799090e-03</td>\n",
       "      <td>1.532603e-02</td>\n",
       "      <td>2.427684e-02</td>\n",
       "      <td>4.111136e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>7.750611e-04</td>\n",
       "      <td>2.060542e-03</td>\n",
       "      <td>1.664065e-03</td>\n",
       "      <td>9.642235e-03</td>\n",
       "      <td>1.713376e-03</td>\n",
       "      <td>1.565868e-03</td>\n",
       "      <td>1.098450e-03</td>\n",
       "      <td>1.889294e-03</td>\n",
       "      <td>1.235315e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.190984e-306</td>\n",
       "      <td>1.572890e-305</td>\n",
       "      <td>1.666965e-305</td>\n",
       "      <td>8.542622e-306</td>\n",
       "      <td>5.309188e-307</td>\n",
       "      <td>6.200085e-306</td>\n",
       "      <td>2.160959e-306</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.182230e-305</td>\n",
       "      <td>5.088318e-305</td>\n",
       "      <td>...</td>\n",
       "      <td>3.103458e-307</td>\n",
       "      <td>1.152121e-306</td>\n",
       "      <td>4.320584e-307</td>\n",
       "      <td>4.865545e-307</td>\n",
       "      <td>1.660531e-306</td>\n",
       "      <td>1.586303e-306</td>\n",
       "      <td>2.187228e-307</td>\n",
       "      <td>6.287674e-307</td>\n",
       "      <td>8.428811e-307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25896</th>\n",
       "      <td>1.320379e-02</td>\n",
       "      <td>3.576054e-02</td>\n",
       "      <td>2.400560e-02</td>\n",
       "      <td>1.242579e-02</td>\n",
       "      <td>1.390931e-03</td>\n",
       "      <td>1.207024e-02</td>\n",
       "      <td>4.813511e-03</td>\n",
       "      <td>1.744991e-02</td>\n",
       "      <td>2.354585e-01</td>\n",
       "      <td>3.799453e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.577460e-04</td>\n",
       "      <td>1.761583e-03</td>\n",
       "      <td>1.777248e-03</td>\n",
       "      <td>1.608226e-03</td>\n",
       "      <td>4.733079e-03</td>\n",
       "      <td>1.968536e-03</td>\n",
       "      <td>5.950869e-04</td>\n",
       "      <td>1.239214e-03</td>\n",
       "      <td>1.355423e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25897</th>\n",
       "      <td>4.529771e-03</td>\n",
       "      <td>1.179237e-02</td>\n",
       "      <td>5.079875e-03</td>\n",
       "      <td>2.026990e-03</td>\n",
       "      <td>4.415183e-04</td>\n",
       "      <td>3.359419e-03</td>\n",
       "      <td>1.820484e-03</td>\n",
       "      <td>2.962209e-03</td>\n",
       "      <td>5.951839e-03</td>\n",
       "      <td>5.865877e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344190e-04</td>\n",
       "      <td>3.964906e-04</td>\n",
       "      <td>7.053673e-04</td>\n",
       "      <td>1.414617e-03</td>\n",
       "      <td>4.209857e-04</td>\n",
       "      <td>3.494251e-04</td>\n",
       "      <td>4.217197e-04</td>\n",
       "      <td>3.731257e-04</td>\n",
       "      <td>3.070442e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25898</th>\n",
       "      <td>1.553569e-02</td>\n",
       "      <td>3.046587e-02</td>\n",
       "      <td>4.626328e-02</td>\n",
       "      <td>2.650033e-01</td>\n",
       "      <td>2.475263e-03</td>\n",
       "      <td>1.914384e-02</td>\n",
       "      <td>6.250478e-03</td>\n",
       "      <td>2.405524e-02</td>\n",
       "      <td>4.629650e-02</td>\n",
       "      <td>2.907926e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.043547e-03</td>\n",
       "      <td>2.478488e-03</td>\n",
       "      <td>1.313843e-03</td>\n",
       "      <td>1.202768e-03</td>\n",
       "      <td>6.112104e-03</td>\n",
       "      <td>5.255948e-03</td>\n",
       "      <td>6.175675e-04</td>\n",
       "      <td>1.593530e-03</td>\n",
       "      <td>3.395612e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25899</th>\n",
       "      <td>4.562475e-03</td>\n",
       "      <td>1.180991e-02</td>\n",
       "      <td>5.106683e-03</td>\n",
       "      <td>2.035990e-03</td>\n",
       "      <td>4.444279e-04</td>\n",
       "      <td>3.383648e-03</td>\n",
       "      <td>1.836676e-03</td>\n",
       "      <td>2.973211e-03</td>\n",
       "      <td>5.955003e-03</td>\n",
       "      <td>5.867759e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.362549e-04</td>\n",
       "      <td>3.984161e-04</td>\n",
       "      <td>7.050692e-04</td>\n",
       "      <td>1.407580e-03</td>\n",
       "      <td>4.219986e-04</td>\n",
       "      <td>3.510347e-04</td>\n",
       "      <td>4.273203e-04</td>\n",
       "      <td>3.756459e-04</td>\n",
       "      <td>3.087271e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25900</th>\n",
       "      <td>4.525197e-03</td>\n",
       "      <td>1.177046e-02</td>\n",
       "      <td>5.079315e-03</td>\n",
       "      <td>2.027654e-03</td>\n",
       "      <td>4.418974e-04</td>\n",
       "      <td>3.358756e-03</td>\n",
       "      <td>1.819407e-03</td>\n",
       "      <td>2.961355e-03</td>\n",
       "      <td>5.955968e-03</td>\n",
       "      <td>5.861897e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344152e-04</td>\n",
       "      <td>3.963120e-04</td>\n",
       "      <td>7.075501e-04</td>\n",
       "      <td>1.407354e-03</td>\n",
       "      <td>4.212455e-04</td>\n",
       "      <td>3.494509e-04</td>\n",
       "      <td>4.211917e-04</td>\n",
       "      <td>3.728618e-04</td>\n",
       "      <td>3.070852e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25901 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              4              5  \\\n",
       "0       2.406603e-02   1.631666e-01   2.307655e-02   8.365412e-03   \n",
       "1      4.937321e-306  9.402316e-306  2.077104e-305   1.000000e+00   \n",
       "2       1.609316e-02   3.192536e-02   5.270181e-02   8.974301e-02   \n",
       "3       2.426547e-02   1.562201e-01   2.336913e-02   8.507393e-03   \n",
       "4      6.190984e-306  1.572890e-305  1.666965e-305  8.542622e-306   \n",
       "...              ...            ...            ...            ...   \n",
       "25896   1.320379e-02   3.576054e-02   2.400560e-02   1.242579e-02   \n",
       "25897   4.529771e-03   1.179237e-02   5.079875e-03   2.026990e-03   \n",
       "25898   1.553569e-02   3.046587e-02   4.626328e-02   2.650033e-01   \n",
       "25899   4.562475e-03   1.180991e-02   5.106683e-03   2.035990e-03   \n",
       "25900   4.525197e-03   1.177046e-02   5.079315e-03   2.027654e-03   \n",
       "\n",
       "                   6              7              8              9  \\\n",
       "0       1.044651e-03   1.436761e-02   7.665692e-03   1.521247e-02   \n",
       "1      6.243770e-307  6.344945e-306  1.919045e-306  9.030316e-306   \n",
       "2       2.049910e-03   1.965568e-02   6.258281e-03   2.901898e-02   \n",
       "3       1.069602e-03   1.459541e-02   7.799090e-03   1.532603e-02   \n",
       "4      5.309188e-307  6.200085e-306  2.160959e-306   1.000000e+00   \n",
       "...              ...            ...            ...            ...   \n",
       "25896   1.390931e-03   1.207024e-02   4.813511e-03   1.744991e-02   \n",
       "25897   4.415183e-04   3.359419e-03   1.820484e-03   2.962209e-03   \n",
       "25898   2.475263e-03   1.914384e-02   6.250478e-03   2.405524e-02   \n",
       "25899   4.444279e-04   3.383648e-03   1.836676e-03   2.973211e-03   \n",
       "25900   4.418974e-04   3.358756e-03   1.819407e-03   2.961355e-03   \n",
       "\n",
       "                  11             12  ...            108            109  \\\n",
       "0       2.381878e-02   4.134728e-02  ...   7.608225e-04   2.045898e-03   \n",
       "1      1.043084e-305  8.912841e-306  ...  3.389193e-307  9.537043e-307   \n",
       "2       4.356667e-02   3.244086e-02  ...   1.021346e-03   2.771639e-03   \n",
       "3       2.427684e-02   4.111136e-02  ...   7.750611e-04   2.060542e-03   \n",
       "4      2.182230e-305  5.088318e-305  ...  3.103458e-307  1.152121e-306   \n",
       "...              ...            ...  ...            ...            ...   \n",
       "25896   2.354585e-01   3.799453e-02  ...   6.577460e-04   1.761583e-03   \n",
       "25897   5.951839e-03   5.865877e-03  ...   2.344190e-04   3.964906e-04   \n",
       "25898   4.629650e-02   2.907926e-02  ...   1.043547e-03   2.478488e-03   \n",
       "25899   5.955003e-03   5.867759e-03  ...   2.362549e-04   3.984161e-04   \n",
       "25900   5.955968e-03   5.861897e-03  ...   2.344152e-04   3.963120e-04   \n",
       "\n",
       "                 111            112            115            116  \\\n",
       "0       1.605185e-03   8.505731e-03   1.680891e-03   1.544089e-03   \n",
       "1      3.224686e-307  3.406447e-307  1.134362e-306  4.168658e-306   \n",
       "2       1.221447e-03   1.183277e-03   5.492027e-03   6.932800e-03   \n",
       "3       1.664065e-03   9.642235e-03   1.713376e-03   1.565868e-03   \n",
       "4      4.320584e-307  4.865545e-307  1.660531e-306  1.586303e-306   \n",
       "...              ...            ...            ...            ...   \n",
       "25896   1.777248e-03   1.608226e-03   4.733079e-03   1.968536e-03   \n",
       "25897   7.053673e-04   1.414617e-03   4.209857e-04   3.494251e-04   \n",
       "25898   1.313843e-03   1.202768e-03   6.112104e-03   5.255948e-03   \n",
       "25899   7.050692e-04   1.407580e-03   4.219986e-04   3.510347e-04   \n",
       "25900   7.075501e-04   1.407354e-03   4.212455e-04   3.494509e-04   \n",
       "\n",
       "                 117            118            119  sentiment  \n",
       "0       1.067855e-03   1.870928e-03   1.215840e-03          0  \n",
       "1      1.944866e-307  5.278984e-307  1.474514e-306          1  \n",
       "2       5.895996e-04   1.672702e-03   3.622286e-03          0  \n",
       "3       1.098450e-03   1.889294e-03   1.235315e-03          0  \n",
       "4      2.187228e-307  6.287674e-307  8.428811e-307          1  \n",
       "...              ...            ...            ...        ...  \n",
       "25896   5.950869e-04   1.239214e-03   1.355423e-03          0  \n",
       "25897   4.217197e-04   3.731257e-04   3.070442e-04          1  \n",
       "25898   6.175675e-04   1.593530e-03   3.395612e-03          1  \n",
       "25899   4.273203e-04   3.756459e-04   3.087271e-04          0  \n",
       "25900   4.211917e-04   3.728618e-04   3.070852e-04          0  \n",
       "\n",
       "[25901 rows x 94 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Bert_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the bag of words\n",
    "vectorizer_model = CountVectorizer(min_df = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the columns in X and y\n",
    "bow_X = vectorizer_model.fit_transform(df[\"review_text\"].values)\n",
    "bow_y = df[\"review_score\"].values\n",
    "STM_X = df_STM.drop(\"sentiment\", axis=1)\n",
    "STM_X = STM_X.values\n",
    "STM_y = df_STM[\"sentiment\"].values\n",
    "STM_X_selected = df_STM_selected.drop(\"sentiment\", axis = 1)\n",
    "STM_y_selected = df_STM_selected[\"sentiment\"].values\n",
    "Bert_X = df_Bert.drop([\"sentiment\", \"Unnamed: 0\"], axis=1)\n",
    "Bert_X = Bert_X.values\n",
    "Bert_y = df_Bert[\"sentiment\"].values\n",
    "Bert_X_selected = df_Bert_selected.drop(\"sentiment\", axis = 1)\n",
    "Bert_y_selected = df_Bert_selected[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25890x16124 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1985300 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the sets\n",
    "bow_X_train, bow_X_test, bow_y_train, bow_y_test = train_test_split(bow_X, bow_y, random_state = 101)\n",
    "STM_X_train, STM_X_test, STM_y_train, STM_y_test = train_test_split(STM_X, STM_y, random_state = 101)\n",
    "Bert_X_train, Bert_X_test, Bert_y_train, Bert_y_test = train_test_split(Bert_X, Bert_y, random_state = 101)\n",
    "STM_X_selected_train, STM_X_selected_test, STM_y_selected_train, STM_y_selected_test = train_test_split(STM_X_selected,STM_y_selected, random_state=101)\n",
    "Bert_X_selected_train, Bert_X_selected_test, Bert_y_selected_train, Bert_y_selected_test = train_test_split(Bert_X_selected,Bert_y_selected, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bow = preprocessing.LabelEncoder()\n",
    "bow_y_train = le_bow.fit_transform(bow_y_train)\n",
    "bow_y_test = le_bow.transform(bow_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_stm = preprocessing.LabelEncoder()\n",
    "STM_y_train = le_stm.fit_transform(STM_y_train)\n",
    "STM_y_test = le_stm.transform(STM_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bert = preprocessing.LabelEncoder()\n",
    "Bert_y_train = le_bert.fit_transform(Bert_y_train)\n",
    "Bert_y_test = le_bert.transform(Bert_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_stm_selected = preprocessing.LabelEncoder()\n",
    "STM_y_selected_train = le_stm.fit_transform(STM_y_selected_train)\n",
    "STM_y_selected_test = le_stm.transform(STM_y_selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bert_selected = preprocessing.LabelEncoder()\n",
    "Bert_y_selected_train = le_bert.fit_transform(Bert_y_selected_train)\n",
    "Bert_y_selected_test = le_bert.transform(Bert_y_selected_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=101, max_iter=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_LR = {\"C\": np.logspace(-4, 4, 20), \"class_weight\":[None, \"balanced\"], \"solver\": [\"liblinear\", \"lbfgs\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=0.004832930238571752, class_weight='balanced',\n",
      "                   max_iter=1000, random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.732832333973113\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'lbfgs', 'class_weight': 'balanced', 'C': 0.004832930238571752}\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_LR_BOW = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_BOW.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_BOW.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_BOW.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_BOW.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.740098429810171"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "LR_final_BOW = Randomized_search_LR_BOW.best_estimator_\n",
    "LR_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_LR = LR_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=206.913808111479, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6935608799722481\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'lbfgs', 'class_weight': 'balanced', 'C': 206.913808111479}\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_LR_STM = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6927320700407771"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "LR_final_STM = Randomized_search_LR_STM.best_estimator_\n",
    "LR_final_STM.fit(STM_X_train, STM_y_train)\n",
    "STM_y_pred_LR = LR_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, STM_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=206.913808111479, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101, solver='liblinear')\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6920081337915035\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 206.913808111479}\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_LR_STM_selected = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6910490856592877"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "LR_final_STM_selected = Randomized_search_LR_STM_selected.best_estimator_\n",
    "LR_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_LR = LR_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=0.004832930238571752, class_weight='balanced',\n",
      "                   max_iter=1000, random_state=101, solver='liblinear')\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.4330325158280076\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 0.004832930238571752}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_LR_Bert = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4268719384184745"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final 0.48 veel features, 0.42 outlier reduction\n",
    "LR_final_Bert = Randomized_search_LR_Bert.best_estimator_\n",
    "LR_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_LR = LR_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=0.0018329807108324356, class_weight='balanced',\n",
      "                   max_iter=1000, random_state=101, solver='liblinear')\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.4301687813309484\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 0.0018329807108324356}\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_LR_Bert_selected = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42118047121690555"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "LR_final_Bert_selected = Randomized_search_LR_Bert_selected.best_estimator_\n",
    "LR_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_LR = LR_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_LR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Support vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = LinearSVC(random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_SVM = {\"C\": np.logspace(-4, 4, 20), \"class_weight\":[None, \"balanced\"], \"loss\": [\"hinge\", \"squared_hinge\"]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=0.08858667904100823, class_weight='balanced', loss='hinge',\n",
      "          random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.7111118894778182\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'hinge', 'class_weight': 'balanced', 'C': 0.08858667904100823}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_SVM_BOW = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_BOW.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_BOW.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_BOW.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_BOW.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7160375090165905"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "SVM_final_BOW = Randomized_search_SVM_BOW.best_estimator_\n",
    "SVM_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_SVM = SVM_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=4.281332398719396, class_weight='balanced', random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6914690436096311\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 4.281332398719396}\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_SVM_STM = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6893203883495146"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "SVM_final_STM = Randomized_search_SVM_STM.best_estimator_\n",
    "SVM_final_STM.fit(STM_X_train, STM_y_train)\n",
    "STM_y_pred_SVM = SVM_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, STM_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=1.623776739188721, class_weight='balanced', random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6897605376201437\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 1.623776739188721}\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_SVM_STM_selected = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6882295719844359"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "SVM_final_STM_selected = Randomized_search_SVM_STM_selected.best_estimator_\n",
    "SVM_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_SVM = SVM_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=0.0001, class_weight='balanced', random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.436066395322654\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_SVM_Bert = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42896299680219274"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final 0.47, 0.42 reduction\n",
    "SVM_final_Bert = Randomized_search_SVM_Bert.best_estimator_\n",
    "SVM_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_SVM = SVM_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=0.0018329807108324356, class_weight='balanced', loss='hinge',\n",
      "          random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.45925395981177325\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'hinge', 'class_weight': 'balanced', 'C': 0.0018329807108324356}\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_SVM_Bert_selected = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44657332350773765"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "SVM_final_Bert_selected = Randomized_search_SVM_Bert_selected.best_estimator_\n",
    "SVM_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_SVM = SVM_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(verbosity = 1, seed = 101, use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters_XGB = {\"colsample_bytree:\": np.arange(0.5,1,0.1) ,\"min_child_weight\": np.arange(1,10,1), \"eta\": np.arange(0.01,0.3,0.05), \"gamma\": np.arange(0,5,1), \"max_depth\": np.arange(3,10,1), \"subsample\": np.arange(0.5,1,0.1), \"scale_pos_weight\": [1, 4.045069258], \"objective\": [\"binary:logistic\", \"binary:logitraw\", \"binary:hinge\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[16:35:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.6,\n",
      "              enable_categorical=False, eta=0.21000000000000002, gamma=2,\n",
      "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.209999993, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='binary:logitraw', predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.7999999999999999, tree_method='exact', ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6846882293823066\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.7999999999999999, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logitraw', 'min_child_weight': 6, 'max_depth': 3, 'gamma': 2, 'eta': 0.21000000000000002, 'colsample_bytree:': 0.6}\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_XGB = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6908167974157822"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "XGB_final_BOW = Randomized_search_XGB.best_estimator_\n",
    "XGB_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_XGB = XGB_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[16:41:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:41:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.7,\n",
      "              enable_categorical=False, eta=0.16000000000000003, gamma=4,\n",
      "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.159999996, max_delta_step=0, max_depth=4,\n",
      "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='binary:logitraw', predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.5, tree_method='exact', ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6989255111574881\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.5, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logitraw', 'min_child_weight': 4, 'max_depth': 4, 'gamma': 4, 'eta': 0.16000000000000003, 'colsample_bytree:': 0.7}\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_XGB_STM = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:42:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7020621000237023"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "XGB_final_STM = Randomized_search_XGB_STM.best_estimator_\n",
    "XGB_final_STM.fit(STM_X_train, STM_y_train)\n",
    "XGB_y_pred_STM = XGB_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, XGB_y_pred_STM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:49:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.6,\n",
      "              enable_categorical=False, eta=0.060000000000000005, gamma=4,\n",
      "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0599999987, max_delta_step=0, max_depth=9,\n",
      "              min_child_weight=2, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              predictor='auto', random_state=101, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=4.045069258, seed=101, subsample=0.7,\n",
      "              tree_method='exact', use_label_encoder=False, ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.7009477395938339\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.7, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 2, 'max_depth': 9, 'gamma': 4, 'eta': 0.060000000000000005, 'colsample_bytree:': 0.6}\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_XGB_STM_selected = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:49:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7057449789817841"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "XGB_final_STM_selected = Randomized_search_XGB_STM_selected.best_estimator_\n",
    "XGB_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_XGB = XGB_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[16:55:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\", \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1,\n",
      "              colsample_bytree:=0.7999999999999999, enable_categorical=False,\n",
      "              eta=0.01, gamma=4, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.00999999978,\n",
      "              max_delta_step=0, max_depth=7, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
      "              num_parallel_tree=1, objective='binary:hinge', predictor='auto',\n",
      "              random_state=101, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              seed=101, subsample=0.6, tree_method='exact', ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.47377838999563976\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.6, 'scale_pos_weight': 1, 'objective': 'binary:hinge', 'min_child_weight': 4, 'max_depth': 7, 'gamma': 4, 'eta': 0.01, 'colsample_bytree:': 0.7999999999999999}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_XGB_Bert = RandomizedSearchCV(XGB, parameters_XGB, verbose=2, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:55:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\", \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47160214749929363"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final\n",
    "XGB_final_Bert = Randomized_search_XGB_Bert.best_estimator_\n",
    "XGB_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_XGB = XGB_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:59:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:59:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1,\n",
      "              colsample_bytree:=0.8999999999999999, enable_categorical=False,\n",
      "              eta=0.11, gamma=1, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.109999999,\n",
      "              max_delta_step=0, max_depth=5, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.5, tree_method='exact', use_label_encoder=False, ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.4713143919902235\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.5, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 4, 'max_depth': 5, 'gamma': 1, 'eta': 0.11, 'colsample_bytree:': 0.8999999999999999}\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_XGB_Bert_selected = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:59:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:59:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4732018883643433"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "XGB_final_Bert_selected = Randomized_search_XGB_Bert_selected.best_estimator_\n",
    "XGB_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_XGB = XGB_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_XGB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
