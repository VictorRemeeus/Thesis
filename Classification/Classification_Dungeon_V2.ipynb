{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter searching: https://www.projectpro.io/recipes/find-optimal-parameters-using-gridsearchcv \\\n",
    "Renaming the last column: https://stackoverflow.com/questions/56479835/rename-only-the-last-column-in-pandas-dataframe-accounting-for-duplicate-header"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in dataset normal, STM and BerTopic\n",
    "df = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Dungeon\\\\dungeon_final.csv\")\n",
    "df_STM = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Dungeon\\\\features_Dungeon_STM.csv\")\n",
    "df_Bert = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Dungeon\\\\Features_Dungeon_Bert_reduction.csv\")\n",
    "df_STM_selected = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Dungeon\\\\Dungeon_featured_selected_STM.csv\")\n",
    "df_Bert_selected = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Dungeon\\\\Dungeon_featured_selected_Bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_STM.columns = [*df_STM.columns[:-1], 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V112</th>\n",
       "      <th>V113</th>\n",
       "      <th>V114</th>\n",
       "      <th>V115</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V118</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.013087</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.011444</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.014052</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.048781</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23535</th>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047478</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.036083</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23536</th>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.492255</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.004326</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23537</th>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.031947</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.030750</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.015561</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23538</th>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.408097</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23539</th>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.012687</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.151728</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23540 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      0.001089  0.009702  0.020566  0.013084  0.002935  0.000625  0.013087   \n",
       "1      0.000899  0.011444  0.007819  0.013961  0.002293  0.000377  0.009079   \n",
       "2      0.001761  0.008275  0.005606  0.011038  0.001675  0.000308  0.004883   \n",
       "3      0.001186  0.004810  0.007594  0.048781  0.001574  0.000424  0.004308   \n",
       "4      0.001220  0.002610  0.004544  0.011449  0.002132  0.000459  0.004296   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23535  0.000675  0.013281  0.003671  0.008411  0.013757  0.013231  0.004524   \n",
       "23536  0.000664  0.004535  0.006504  0.020917  0.004546  0.000573  0.007687   \n",
       "23537  0.001303  0.031947  0.005099  0.030750  0.006701  0.001019  0.011551   \n",
       "23538  0.000634  0.005594  0.006251  0.017025  0.002473  0.000660  0.016071   \n",
       "23539  0.001681  0.012687  0.005587  0.022068  0.003894  0.000686  0.017900   \n",
       "\n",
       "             V8        V9       V10  ...      V112      V113      V114  \\\n",
       "0      0.000229  0.008449  0.001161  ...  0.001070  0.000228  0.000813   \n",
       "1      0.000149  0.014052  0.001108  ...  0.003235  0.000240  0.004049   \n",
       "2      0.000167  0.012181  0.000977  ...  0.006799  0.000210  0.003072   \n",
       "3      0.000142  0.013870  0.000853  ...  0.001974  0.000187  0.003104   \n",
       "4      0.000327  0.013348  0.000791  ...  0.009786  0.000138  0.002863   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23535  0.000430  0.006005  0.000878  ...  0.047478  0.001388  0.000604   \n",
       "23536  0.000187  0.019356  0.000952  ...  0.004701  0.492255  0.001260   \n",
       "23537  0.000625  0.015561  0.001541  ...  0.002134  0.001300  0.002335   \n",
       "23538  0.000136  0.011134  0.000974  ...  0.003673  0.408097  0.000732   \n",
       "23539  0.000176  0.008053  0.001412  ...  0.005389  0.151728  0.000567   \n",
       "\n",
       "           V115      V116      V117      V118      V119      V120  sentiment  \n",
       "0      0.000244  0.001883  0.006174  0.000789  0.000627  0.002293          0  \n",
       "1      0.000207  0.002518  0.002555  0.000498  0.001693  0.002264          0  \n",
       "2      0.000522  0.002364  0.001739  0.000405  0.001074  0.002432          0  \n",
       "3      0.000359  0.001884  0.003372  0.000564  0.001806  0.002076          0  \n",
       "4      0.000765  0.004285  0.002055  0.000905  0.001165  0.002796          0  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "23535  0.000614  0.036083  0.001506  0.002012  0.000300  0.001842          1  \n",
       "23536  0.000382  0.004326  0.001039  0.000834  0.000177  0.001741          1  \n",
       "23537  0.000222  0.003114  0.002778  0.001109  0.000347  0.002301          1  \n",
       "23538  0.000177  0.008117  0.000524  0.001079  0.000325  0.001473          0  \n",
       "23539  0.000089  0.009277  0.000400  0.001817  0.000697  0.001946          0  \n",
       "\n",
       "[23540 rows x 121 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_STM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.554550e-02</td>\n",
       "      <td>1.081630e-01</td>\n",
       "      <td>5.663138e-02</td>\n",
       "      <td>2.333239e-03</td>\n",
       "      <td>3.596339e-02</td>\n",
       "      <td>2.098619e-03</td>\n",
       "      <td>2.364012e-02</td>\n",
       "      <td>5.254515e-03</td>\n",
       "      <td>6.838805e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.165053e-03</td>\n",
       "      <td>4.567182e-03</td>\n",
       "      <td>3.278959e-03</td>\n",
       "      <td>2.556096e-03</td>\n",
       "      <td>2.139480e-03</td>\n",
       "      <td>1.673649e-03</td>\n",
       "      <td>2.630820e-03</td>\n",
       "      <td>3.593357e-03</td>\n",
       "      <td>1.478194e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.519406e-03</td>\n",
       "      <td>8.169974e-03</td>\n",
       "      <td>2.608046e-02</td>\n",
       "      <td>9.225114e-04</td>\n",
       "      <td>3.512590e-02</td>\n",
       "      <td>1.206870e-03</td>\n",
       "      <td>2.487019e-02</td>\n",
       "      <td>2.956705e-03</td>\n",
       "      <td>5.401715e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.102972e-04</td>\n",
       "      <td>5.733551e-03</td>\n",
       "      <td>4.230669e-03</td>\n",
       "      <td>2.152340e-03</td>\n",
       "      <td>1.521066e-03</td>\n",
       "      <td>1.196298e-03</td>\n",
       "      <td>2.406235e-03</td>\n",
       "      <td>3.932182e-03</td>\n",
       "      <td>4.843089e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.509663e-307</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.784100e-306</td>\n",
       "      <td>1.556598e-307</td>\n",
       "      <td>2.065353e-306</td>\n",
       "      <td>1.298450e-307</td>\n",
       "      <td>1.291607e-306</td>\n",
       "      <td>2.963634e-307</td>\n",
       "      <td>4.050248e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>7.147302e-308</td>\n",
       "      <td>2.582192e-307</td>\n",
       "      <td>1.791893e-307</td>\n",
       "      <td>1.400413e-307</td>\n",
       "      <td>1.195653e-307</td>\n",
       "      <td>1.024952e-307</td>\n",
       "      <td>1.576184e-307</td>\n",
       "      <td>1.925494e-307</td>\n",
       "      <td>9.259370e-308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.051417e-306</td>\n",
       "      <td>2.016642e-306</td>\n",
       "      <td>9.047224e-306</td>\n",
       "      <td>1.369693e-307</td>\n",
       "      <td>4.872387e-306</td>\n",
       "      <td>1.601557e-307</td>\n",
       "      <td>2.919845e-306</td>\n",
       "      <td>4.303837e-307</td>\n",
       "      <td>6.876297e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>9.257109e-308</td>\n",
       "      <td>7.845563e-307</td>\n",
       "      <td>4.551687e-307</td>\n",
       "      <td>2.742387e-307</td>\n",
       "      <td>2.091530e-307</td>\n",
       "      <td>1.497740e-307</td>\n",
       "      <td>2.953929e-307</td>\n",
       "      <td>4.720487e-307</td>\n",
       "      <td>7.251976e-308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.707980e-306</td>\n",
       "      <td>5.874200e-307</td>\n",
       "      <td>1.161093e-306</td>\n",
       "      <td>8.777461e-308</td>\n",
       "      <td>1.801795e-306</td>\n",
       "      <td>1.510206e-307</td>\n",
       "      <td>3.794988e-306</td>\n",
       "      <td>7.456014e-307</td>\n",
       "      <td>5.806310e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>9.384838e-308</td>\n",
       "      <td>2.244154e-307</td>\n",
       "      <td>4.294892e-307</td>\n",
       "      <td>4.862144e-307</td>\n",
       "      <td>2.677420e-307</td>\n",
       "      <td>1.300161e-307</td>\n",
       "      <td>1.668917e-307</td>\n",
       "      <td>7.690572e-307</td>\n",
       "      <td>6.718824e-308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>23560</td>\n",
       "      <td>7.206797e-03</td>\n",
       "      <td>4.045883e-03</td>\n",
       "      <td>8.242443e-03</td>\n",
       "      <td>7.917606e-04</td>\n",
       "      <td>1.409944e-02</td>\n",
       "      <td>1.749929e-03</td>\n",
       "      <td>3.167182e-02</td>\n",
       "      <td>1.859028e-02</td>\n",
       "      <td>6.202524e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.161423e-03</td>\n",
       "      <td>1.671172e-03</td>\n",
       "      <td>3.111742e-03</td>\n",
       "      <td>6.347289e-03</td>\n",
       "      <td>4.234082e-03</td>\n",
       "      <td>1.459434e-03</td>\n",
       "      <td>1.529516e-03</td>\n",
       "      <td>3.492920e-03</td>\n",
       "      <td>6.021542e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23561</th>\n",
       "      <td>23561</td>\n",
       "      <td>3.115431e-04</td>\n",
       "      <td>1.818786e-04</td>\n",
       "      <td>3.693197e-04</td>\n",
       "      <td>2.805660e-05</td>\n",
       "      <td>6.146754e-04</td>\n",
       "      <td>6.753109e-05</td>\n",
       "      <td>1.191088e-03</td>\n",
       "      <td>1.273958e-03</td>\n",
       "      <td>2.812027e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.278478e-05</td>\n",
       "      <td>7.177979e-05</td>\n",
       "      <td>1.207654e-04</td>\n",
       "      <td>2.146819e-04</td>\n",
       "      <td>2.141143e-04</td>\n",
       "      <td>7.620899e-05</td>\n",
       "      <td>7.412592e-05</td>\n",
       "      <td>1.261484e-04</td>\n",
       "      <td>2.136162e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23562</th>\n",
       "      <td>23562</td>\n",
       "      <td>8.326945e-307</td>\n",
       "      <td>5.434685e-307</td>\n",
       "      <td>1.179855e-306</td>\n",
       "      <td>9.456703e-308</td>\n",
       "      <td>2.204470e-306</td>\n",
       "      <td>1.831498e-307</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.626891e-307</td>\n",
       "      <td>1.082098e-306</td>\n",
       "      <td>...</td>\n",
       "      <td>1.186582e-307</td>\n",
       "      <td>2.706973e-307</td>\n",
       "      <td>1.265684e-306</td>\n",
       "      <td>1.189955e-306</td>\n",
       "      <td>4.832029e-307</td>\n",
       "      <td>1.863678e-307</td>\n",
       "      <td>2.391894e-307</td>\n",
       "      <td>1.040852e-306</td>\n",
       "      <td>6.045012e-308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>23563</td>\n",
       "      <td>3.114336e-04</td>\n",
       "      <td>1.817410e-04</td>\n",
       "      <td>3.690426e-04</td>\n",
       "      <td>2.801808e-05</td>\n",
       "      <td>6.141782e-04</td>\n",
       "      <td>6.737864e-05</td>\n",
       "      <td>1.192123e-03</td>\n",
       "      <td>1.269790e-03</td>\n",
       "      <td>2.810003e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.265604e-05</td>\n",
       "      <td>7.173171e-05</td>\n",
       "      <td>1.208083e-04</td>\n",
       "      <td>2.151753e-04</td>\n",
       "      <td>2.140637e-04</td>\n",
       "      <td>7.609164e-05</td>\n",
       "      <td>7.406042e-05</td>\n",
       "      <td>1.262463e-04</td>\n",
       "      <td>2.132820e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23564</th>\n",
       "      <td>23564</td>\n",
       "      <td>3.115807e-04</td>\n",
       "      <td>1.814471e-04</td>\n",
       "      <td>3.683721e-04</td>\n",
       "      <td>2.791553e-05</td>\n",
       "      <td>6.126259e-04</td>\n",
       "      <td>6.691020e-05</td>\n",
       "      <td>1.193420e-03</td>\n",
       "      <td>1.272835e-03</td>\n",
       "      <td>2.799407e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.223955e-05</td>\n",
       "      <td>7.158518e-05</td>\n",
       "      <td>1.208320e-04</td>\n",
       "      <td>2.161167e-04</td>\n",
       "      <td>2.129157e-04</td>\n",
       "      <td>7.566935e-05</td>\n",
       "      <td>7.382973e-05</td>\n",
       "      <td>1.265369e-04</td>\n",
       "      <td>2.125595e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23565 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0              0              1              2              3  \\\n",
       "0               0   1.554550e-02   1.081630e-01   5.663138e-02   2.333239e-03   \n",
       "1               1   6.519406e-03   8.169974e-03   2.608046e-02   9.225114e-04   \n",
       "2               2  8.509663e-307   1.000000e+00  3.784100e-306  1.556598e-307   \n",
       "3               3  1.051417e-306  2.016642e-306  9.047224e-306  1.369693e-307   \n",
       "4               4  1.707980e-306  5.874200e-307  1.161093e-306  8.777461e-308   \n",
       "...           ...            ...            ...            ...            ...   \n",
       "23560       23560   7.206797e-03   4.045883e-03   8.242443e-03   7.917606e-04   \n",
       "23561       23561   3.115431e-04   1.818786e-04   3.693197e-04   2.805660e-05   \n",
       "23562       23562  8.326945e-307  5.434685e-307  1.179855e-306  9.456703e-308   \n",
       "23563       23563   3.114336e-04   1.817410e-04   3.690426e-04   2.801808e-05   \n",
       "23564       23564   3.115807e-04   1.814471e-04   3.683721e-04   2.791553e-05   \n",
       "\n",
       "                   4              5              6              7  \\\n",
       "0       3.596339e-02   2.098619e-03   2.364012e-02   5.254515e-03   \n",
       "1       3.512590e-02   1.206870e-03   2.487019e-02   2.956705e-03   \n",
       "2      2.065353e-306  1.298450e-307  1.291607e-306  2.963634e-307   \n",
       "3      4.872387e-306  1.601557e-307  2.919845e-306  4.303837e-307   \n",
       "4      1.801795e-306  1.510206e-307  3.794988e-306  7.456014e-307   \n",
       "...              ...            ...            ...            ...   \n",
       "23560   1.409944e-02   1.749929e-03   3.167182e-02   1.859028e-02   \n",
       "23561   6.146754e-04   6.753109e-05   1.191088e-03   1.273958e-03   \n",
       "23562  2.204470e-306  1.831498e-307   1.000000e+00  6.626891e-307   \n",
       "23563   6.141782e-04   6.737864e-05   1.192123e-03   1.269790e-03   \n",
       "23564   6.126259e-04   6.691020e-05   1.193420e-03   1.272835e-03   \n",
       "\n",
       "                   8  ...            111            112            113  \\\n",
       "0       6.838805e-03  ...   1.165053e-03   4.567182e-03   3.278959e-03   \n",
       "1       5.401715e-03  ...   7.102972e-04   5.733551e-03   4.230669e-03   \n",
       "2      4.050248e-307  ...  7.147302e-308  2.582192e-307  1.791893e-307   \n",
       "3      6.876297e-307  ...  9.257109e-308  7.845563e-307  4.551687e-307   \n",
       "4      5.806310e-307  ...  9.384838e-308  2.244154e-307  4.294892e-307   \n",
       "...              ...  ...            ...            ...            ...   \n",
       "23560   6.202524e-03  ...   1.161423e-03   1.671172e-03   3.111742e-03   \n",
       "23561   2.812027e-04  ...   5.278478e-05   7.177979e-05   1.207654e-04   \n",
       "23562  1.082098e-306  ...  1.186582e-307  2.706973e-307  1.265684e-306   \n",
       "23563   2.810003e-04  ...   5.265604e-05   7.173171e-05   1.208083e-04   \n",
       "23564   2.799407e-04  ...   5.223955e-05   7.158518e-05   1.208320e-04   \n",
       "\n",
       "                 114            115            116            117  \\\n",
       "0       2.556096e-03   2.139480e-03   1.673649e-03   2.630820e-03   \n",
       "1       2.152340e-03   1.521066e-03   1.196298e-03   2.406235e-03   \n",
       "2      1.400413e-307  1.195653e-307  1.024952e-307  1.576184e-307   \n",
       "3      2.742387e-307  2.091530e-307  1.497740e-307  2.953929e-307   \n",
       "4      4.862144e-307  2.677420e-307  1.300161e-307  1.668917e-307   \n",
       "...              ...            ...            ...            ...   \n",
       "23560   6.347289e-03   4.234082e-03   1.459434e-03   1.529516e-03   \n",
       "23561   2.146819e-04   2.141143e-04   7.620899e-05   7.412592e-05   \n",
       "23562  1.189955e-306  4.832029e-307  1.863678e-307  2.391894e-307   \n",
       "23563   2.151753e-04   2.140637e-04   7.609164e-05   7.406042e-05   \n",
       "23564   2.161167e-04   2.129157e-04   7.566935e-05   7.382973e-05   \n",
       "\n",
       "                 118            119  sentiment  \n",
       "0       3.593357e-03   1.478194e-03          0  \n",
       "1       3.932182e-03   4.843089e-04          0  \n",
       "2      1.925494e-307  9.259370e-308          0  \n",
       "3      4.720487e-307  7.251976e-308          0  \n",
       "4      7.690572e-307  6.718824e-308          0  \n",
       "...              ...            ...        ...  \n",
       "23560   3.492920e-03   6.021542e-04          1  \n",
       "23561   1.261484e-04   2.136162e-05          1  \n",
       "23562  1.040852e-306  6.045012e-308          1  \n",
       "23563   1.262463e-04   2.132820e-05          0  \n",
       "23564   1.265369e-04   2.125595e-05          0  \n",
       "\n",
       "[23565 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>V110</th>\n",
       "      <th>V111</th>\n",
       "      <th>V112</th>\n",
       "      <th>V113</th>\n",
       "      <th>V115</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.013087</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049348</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.011444</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103904</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029561</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.048781</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010229</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23535</th>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013332</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.047478</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.036083</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23536</th>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.492255</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.004326</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23537</th>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.031947</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.030750</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009674</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23538</th>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.012201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019680</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.408097</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23539</th>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.012687</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.151728</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23540 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      0.001089  0.009702  0.020566  0.013084  0.002935  0.000625  0.013087   \n",
       "1      0.000899  0.011444  0.007819  0.013961  0.002293  0.000377  0.009079   \n",
       "2      0.001761  0.008275  0.005606  0.011038  0.001675  0.000308  0.004883   \n",
       "3      0.001186  0.004810  0.007594  0.048781  0.001574  0.000424  0.004308   \n",
       "4      0.001220  0.002610  0.004544  0.011449  0.002132  0.000459  0.004296   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23535  0.000675  0.013281  0.003671  0.008411  0.013757  0.013231  0.004524   \n",
       "23536  0.000664  0.004535  0.006504  0.020917  0.004546  0.000573  0.007687   \n",
       "23537  0.001303  0.031947  0.005099  0.030750  0.006701  0.001019  0.011551   \n",
       "23538  0.000634  0.005594  0.006251  0.017025  0.002473  0.000660  0.016071   \n",
       "23539  0.001681  0.012687  0.005587  0.022068  0.003894  0.000686  0.017900   \n",
       "\n",
       "             V8       V10       V11  ...      V110      V111      V112  \\\n",
       "0      0.000229  0.001161  0.005097  ...  0.049348  0.000641  0.001070   \n",
       "1      0.000149  0.001108  0.003534  ...  0.103904  0.001230  0.003235   \n",
       "2      0.000167  0.000977  0.004857  ...  0.029561  0.002055  0.006799   \n",
       "3      0.000142  0.000853  0.002419  ...  0.046800  0.001687  0.001974   \n",
       "4      0.000327  0.000791  0.002498  ...  0.010229  0.002668  0.009786   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23535  0.000430  0.000878  0.006167  ...  0.013332  0.001247  0.047478   \n",
       "23536  0.000187  0.000952  0.014038  ...  0.002956  0.000732  0.004701   \n",
       "23537  0.000625  0.001541  0.005669  ...  0.009674  0.000801  0.002134   \n",
       "23538  0.000136  0.000974  0.012201  ...  0.019680  0.000593  0.003673   \n",
       "23539  0.000176  0.001412  0.005835  ...  0.005597  0.000929  0.005389   \n",
       "\n",
       "           V113      V115      V116      V117      V119      V120  sentiment  \n",
       "0      0.000228  0.000244  0.001883  0.006174  0.000627  0.002293          0  \n",
       "1      0.000240  0.000207  0.002518  0.002555  0.001693  0.002264          0  \n",
       "2      0.000210  0.000522  0.002364  0.001739  0.001074  0.002432          0  \n",
       "3      0.000187  0.000359  0.001884  0.003372  0.001806  0.002076          0  \n",
       "4      0.000138  0.000765  0.004285  0.002055  0.001165  0.002796          0  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "23535  0.001388  0.000614  0.036083  0.001506  0.000300  0.001842          1  \n",
       "23536  0.492255  0.000382  0.004326  0.001039  0.000177  0.001741          1  \n",
       "23537  0.001300  0.000222  0.003114  0.002778  0.000347  0.002301          1  \n",
       "23538  0.408097  0.000177  0.008117  0.000524  0.000325  0.001473          0  \n",
       "23539  0.151728  0.000089  0.009277  0.000400  0.000697  0.001946          0  \n",
       "\n",
       "[23540 rows x 110 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_STM_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.554550e-02</td>\n",
       "      <td>1.081630e-01</td>\n",
       "      <td>5.663138e-02</td>\n",
       "      <td>2.333239e-03</td>\n",
       "      <td>3.596339e-02</td>\n",
       "      <td>2.364012e-02</td>\n",
       "      <td>5.254515e-03</td>\n",
       "      <td>6.838805e-03</td>\n",
       "      <td>8.749989e-03</td>\n",
       "      <td>1.507250e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.837752e-03</td>\n",
       "      <td>4.033641e-03</td>\n",
       "      <td>4.567182e-03</td>\n",
       "      <td>3.278959e-03</td>\n",
       "      <td>2.556096e-03</td>\n",
       "      <td>2.139480e-03</td>\n",
       "      <td>2.630820e-03</td>\n",
       "      <td>3.593357e-03</td>\n",
       "      <td>1.478194e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.519406e-03</td>\n",
       "      <td>8.169974e-03</td>\n",
       "      <td>2.608046e-02</td>\n",
       "      <td>9.225114e-04</td>\n",
       "      <td>3.512590e-02</td>\n",
       "      <td>2.487019e-02</td>\n",
       "      <td>2.956705e-03</td>\n",
       "      <td>5.401715e-03</td>\n",
       "      <td>6.349214e-03</td>\n",
       "      <td>1.602588e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.957609e-03</td>\n",
       "      <td>7.012775e-03</td>\n",
       "      <td>5.733551e-03</td>\n",
       "      <td>4.230669e-03</td>\n",
       "      <td>2.152340e-03</td>\n",
       "      <td>1.521066e-03</td>\n",
       "      <td>2.406235e-03</td>\n",
       "      <td>3.932182e-03</td>\n",
       "      <td>4.843089e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.509663e-307</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.784100e-306</td>\n",
       "      <td>1.556598e-307</td>\n",
       "      <td>2.065353e-306</td>\n",
       "      <td>1.291607e-306</td>\n",
       "      <td>2.963634e-307</td>\n",
       "      <td>4.050248e-307</td>\n",
       "      <td>5.168233e-307</td>\n",
       "      <td>8.602949e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>1.580669e-307</td>\n",
       "      <td>2.248323e-307</td>\n",
       "      <td>2.582192e-307</td>\n",
       "      <td>1.791893e-307</td>\n",
       "      <td>1.400413e-307</td>\n",
       "      <td>1.195653e-307</td>\n",
       "      <td>1.576184e-307</td>\n",
       "      <td>1.925494e-307</td>\n",
       "      <td>9.259370e-308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.051417e-306</td>\n",
       "      <td>2.016642e-306</td>\n",
       "      <td>9.047224e-306</td>\n",
       "      <td>1.369693e-307</td>\n",
       "      <td>4.872387e-306</td>\n",
       "      <td>2.919845e-306</td>\n",
       "      <td>4.303837e-307</td>\n",
       "      <td>6.876297e-307</td>\n",
       "      <td>8.061368e-307</td>\n",
       "      <td>2.002931e-306</td>\n",
       "      <td>...</td>\n",
       "      <td>3.560482e-307</td>\n",
       "      <td>7.166419e-307</td>\n",
       "      <td>7.845563e-307</td>\n",
       "      <td>4.551687e-307</td>\n",
       "      <td>2.742387e-307</td>\n",
       "      <td>2.091530e-307</td>\n",
       "      <td>2.953929e-307</td>\n",
       "      <td>4.720487e-307</td>\n",
       "      <td>7.251976e-308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.707980e-306</td>\n",
       "      <td>5.874200e-307</td>\n",
       "      <td>1.161093e-306</td>\n",
       "      <td>8.777461e-308</td>\n",
       "      <td>1.801795e-306</td>\n",
       "      <td>3.794988e-306</td>\n",
       "      <td>7.456014e-307</td>\n",
       "      <td>5.806310e-307</td>\n",
       "      <td>9.759840e-307</td>\n",
       "      <td>9.346771e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>2.672976e-307</td>\n",
       "      <td>2.534479e-307</td>\n",
       "      <td>2.244154e-307</td>\n",
       "      <td>4.294892e-307</td>\n",
       "      <td>4.862144e-307</td>\n",
       "      <td>2.677420e-307</td>\n",
       "      <td>1.668917e-307</td>\n",
       "      <td>7.690572e-307</td>\n",
       "      <td>6.718824e-308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>7.206797e-03</td>\n",
       "      <td>4.045883e-03</td>\n",
       "      <td>8.242443e-03</td>\n",
       "      <td>7.917606e-04</td>\n",
       "      <td>1.409944e-02</td>\n",
       "      <td>3.167182e-02</td>\n",
       "      <td>1.859028e-02</td>\n",
       "      <td>6.202524e-03</td>\n",
       "      <td>1.439559e-02</td>\n",
       "      <td>8.029826e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.473415e-03</td>\n",
       "      <td>1.917724e-03</td>\n",
       "      <td>1.671172e-03</td>\n",
       "      <td>3.111742e-03</td>\n",
       "      <td>6.347289e-03</td>\n",
       "      <td>4.234082e-03</td>\n",
       "      <td>1.529516e-03</td>\n",
       "      <td>3.492920e-03</td>\n",
       "      <td>6.021542e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23561</th>\n",
       "      <td>3.115431e-04</td>\n",
       "      <td>1.818786e-04</td>\n",
       "      <td>3.693197e-04</td>\n",
       "      <td>2.805660e-05</td>\n",
       "      <td>6.146754e-04</td>\n",
       "      <td>1.191088e-03</td>\n",
       "      <td>1.273958e-03</td>\n",
       "      <td>2.812027e-04</td>\n",
       "      <td>7.925056e-04</td>\n",
       "      <td>3.483617e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.069481e-04</td>\n",
       "      <td>8.197761e-05</td>\n",
       "      <td>7.177979e-05</td>\n",
       "      <td>1.207654e-04</td>\n",
       "      <td>2.146819e-04</td>\n",
       "      <td>2.141143e-04</td>\n",
       "      <td>7.412592e-05</td>\n",
       "      <td>1.261484e-04</td>\n",
       "      <td>2.136162e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23562</th>\n",
       "      <td>8.326945e-307</td>\n",
       "      <td>5.434685e-307</td>\n",
       "      <td>1.179855e-306</td>\n",
       "      <td>9.456703e-308</td>\n",
       "      <td>2.204470e-306</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.626891e-307</td>\n",
       "      <td>1.082098e-306</td>\n",
       "      <td>1.641643e-306</td>\n",
       "      <td>1.236296e-306</td>\n",
       "      <td>...</td>\n",
       "      <td>5.193479e-307</td>\n",
       "      <td>3.492160e-307</td>\n",
       "      <td>2.706973e-307</td>\n",
       "      <td>1.265684e-306</td>\n",
       "      <td>1.189955e-306</td>\n",
       "      <td>4.832029e-307</td>\n",
       "      <td>2.391894e-307</td>\n",
       "      <td>1.040852e-306</td>\n",
       "      <td>6.045012e-308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>3.114336e-04</td>\n",
       "      <td>1.817410e-04</td>\n",
       "      <td>3.690426e-04</td>\n",
       "      <td>2.801808e-05</td>\n",
       "      <td>6.141782e-04</td>\n",
       "      <td>1.192123e-03</td>\n",
       "      <td>1.269790e-03</td>\n",
       "      <td>2.810003e-04</td>\n",
       "      <td>7.912760e-04</td>\n",
       "      <td>3.479749e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.069102e-04</td>\n",
       "      <td>8.193666e-05</td>\n",
       "      <td>7.173171e-05</td>\n",
       "      <td>1.208083e-04</td>\n",
       "      <td>2.151753e-04</td>\n",
       "      <td>2.140637e-04</td>\n",
       "      <td>7.406042e-05</td>\n",
       "      <td>1.262463e-04</td>\n",
       "      <td>2.132820e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23564</th>\n",
       "      <td>3.115807e-04</td>\n",
       "      <td>1.814471e-04</td>\n",
       "      <td>3.683721e-04</td>\n",
       "      <td>2.791553e-05</td>\n",
       "      <td>6.126259e-04</td>\n",
       "      <td>1.193420e-03</td>\n",
       "      <td>1.272835e-03</td>\n",
       "      <td>2.799407e-04</td>\n",
       "      <td>7.851255e-04</td>\n",
       "      <td>3.467117e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066662e-04</td>\n",
       "      <td>8.179009e-05</td>\n",
       "      <td>7.158518e-05</td>\n",
       "      <td>1.208320e-04</td>\n",
       "      <td>2.161167e-04</td>\n",
       "      <td>2.129157e-04</td>\n",
       "      <td>7.382973e-05</td>\n",
       "      <td>1.265369e-04</td>\n",
       "      <td>2.125595e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23565 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              2              3  \\\n",
       "0       1.554550e-02   1.081630e-01   5.663138e-02   2.333239e-03   \n",
       "1       6.519406e-03   8.169974e-03   2.608046e-02   9.225114e-04   \n",
       "2      8.509663e-307   1.000000e+00  3.784100e-306  1.556598e-307   \n",
       "3      1.051417e-306  2.016642e-306  9.047224e-306  1.369693e-307   \n",
       "4      1.707980e-306  5.874200e-307  1.161093e-306  8.777461e-308   \n",
       "...              ...            ...            ...            ...   \n",
       "23560   7.206797e-03   4.045883e-03   8.242443e-03   7.917606e-04   \n",
       "23561   3.115431e-04   1.818786e-04   3.693197e-04   2.805660e-05   \n",
       "23562  8.326945e-307  5.434685e-307  1.179855e-306  9.456703e-308   \n",
       "23563   3.114336e-04   1.817410e-04   3.690426e-04   2.801808e-05   \n",
       "23564   3.115807e-04   1.814471e-04   3.683721e-04   2.791553e-05   \n",
       "\n",
       "                   4              6              7              8  \\\n",
       "0       3.596339e-02   2.364012e-02   5.254515e-03   6.838805e-03   \n",
       "1       3.512590e-02   2.487019e-02   2.956705e-03   5.401715e-03   \n",
       "2      2.065353e-306  1.291607e-306  2.963634e-307  4.050248e-307   \n",
       "3      4.872387e-306  2.919845e-306  4.303837e-307  6.876297e-307   \n",
       "4      1.801795e-306  3.794988e-306  7.456014e-307  5.806310e-307   \n",
       "...              ...            ...            ...            ...   \n",
       "23560   1.409944e-02   3.167182e-02   1.859028e-02   6.202524e-03   \n",
       "23561   6.146754e-04   1.191088e-03   1.273958e-03   2.812027e-04   \n",
       "23562  2.204470e-306   1.000000e+00  6.626891e-307  1.082098e-306   \n",
       "23563   6.141782e-04   1.192123e-03   1.269790e-03   2.810003e-04   \n",
       "23564   6.126259e-04   1.193420e-03   1.272835e-03   2.799407e-04   \n",
       "\n",
       "                  10             11  ...            109            110  \\\n",
       "0       8.749989e-03   1.507250e-02  ...   2.837752e-03   4.033641e-03   \n",
       "1       6.349214e-03   1.602588e-02  ...   2.957609e-03   7.012775e-03   \n",
       "2      5.168233e-307  8.602949e-307  ...  1.580669e-307  2.248323e-307   \n",
       "3      8.061368e-307  2.002931e-306  ...  3.560482e-307  7.166419e-307   \n",
       "4      9.759840e-307  9.346771e-307  ...  2.672976e-307  2.534479e-307   \n",
       "...              ...            ...  ...            ...            ...   \n",
       "23560   1.439559e-02   8.029826e-03  ...   2.473415e-03   1.917724e-03   \n",
       "23561   7.925056e-04   3.483617e-04  ...   1.069481e-04   8.197761e-05   \n",
       "23562  1.641643e-306  1.236296e-306  ...  5.193479e-307  3.492160e-307   \n",
       "23563   7.912760e-04   3.479749e-04  ...   1.069102e-04   8.193666e-05   \n",
       "23564   7.851255e-04   3.467117e-04  ...   1.066662e-04   8.179009e-05   \n",
       "\n",
       "                 112            113            114            115  \\\n",
       "0       4.567182e-03   3.278959e-03   2.556096e-03   2.139480e-03   \n",
       "1       5.733551e-03   4.230669e-03   2.152340e-03   1.521066e-03   \n",
       "2      2.582192e-307  1.791893e-307  1.400413e-307  1.195653e-307   \n",
       "3      7.845563e-307  4.551687e-307  2.742387e-307  2.091530e-307   \n",
       "4      2.244154e-307  4.294892e-307  4.862144e-307  2.677420e-307   \n",
       "...              ...            ...            ...            ...   \n",
       "23560   1.671172e-03   3.111742e-03   6.347289e-03   4.234082e-03   \n",
       "23561   7.177979e-05   1.207654e-04   2.146819e-04   2.141143e-04   \n",
       "23562  2.706973e-307  1.265684e-306  1.189955e-306  4.832029e-307   \n",
       "23563   7.173171e-05   1.208083e-04   2.151753e-04   2.140637e-04   \n",
       "23564   7.158518e-05   1.208320e-04   2.161167e-04   2.129157e-04   \n",
       "\n",
       "                 117            118            119  sentiment  \n",
       "0       2.630820e-03   3.593357e-03   1.478194e-03          0  \n",
       "1       2.406235e-03   3.932182e-03   4.843089e-04          0  \n",
       "2      1.576184e-307  1.925494e-307  9.259370e-308          0  \n",
       "3      2.953929e-307  4.720487e-307  7.251976e-308          0  \n",
       "4      1.668917e-307  7.690572e-307  6.718824e-308          0  \n",
       "...              ...            ...            ...        ...  \n",
       "23560   1.529516e-03   3.492920e-03   6.021542e-04          1  \n",
       "23561   7.412592e-05   1.261484e-04   2.136162e-05          1  \n",
       "23562  2.391894e-307  1.040852e-306  6.045012e-308          1  \n",
       "23563   7.406042e-05   1.262463e-04   2.132820e-05          0  \n",
       "23564   7.382973e-05   1.265369e-04   2.125595e-05          0  \n",
       "\n",
       "[23565 rows x 97 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Bert_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the bag of words\n",
    "vectorizer_model = CountVectorizer(min_df = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the columns in X and y\n",
    "bow_X = vectorizer_model.fit_transform(df[\"review_text\"].values)\n",
    "bow_y = df[\"review_score\"].values\n",
    "STM_X = df_STM.drop(\"sentiment\", axis=1)\n",
    "STM_X = STM_X.values\n",
    "STM_y = df_STM[\"sentiment\"].values\n",
    "STM_X_selected = df_STM_selected.drop(\"sentiment\", axis = 1)\n",
    "STM_y_selected = df_STM_selected[\"sentiment\"].values\n",
    "Bert_X = df_Bert.drop([\"sentiment\", \"Unnamed: 0\"], axis=1)\n",
    "Bert_X = Bert_X.values\n",
    "Bert_y = df_Bert[\"sentiment\"].values\n",
    "Bert_X_selected = df_Bert_selected.drop(\"sentiment\", axis = 1)\n",
    "Bert_y_selected = df_Bert_selected[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23543x13382 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1828569 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the sets\n",
    "bow_X_train, bow_X_test, bow_y_train, bow_y_test = train_test_split(bow_X, bow_y, random_state = 101)\n",
    "STM_X_train, STM_X_test, STM_y_train, STM_y_test = train_test_split(STM_X, STM_y, random_state = 101)\n",
    "Bert_X_train, Bert_X_test, Bert_y_train, Bert_y_test = train_test_split(Bert_X, Bert_y, random_state = 101)\n",
    "STM_X_selected_train, STM_X_selected_test, STM_y_selected_train, STM_y_selected_test = train_test_split(STM_X_selected,STM_y_selected, random_state=101)\n",
    "Bert_X_selected_train, Bert_X_selected_test, Bert_y_selected_train, Bert_y_selected_test = train_test_split(Bert_X_selected,Bert_y_selected, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bow = preprocessing.LabelEncoder()\n",
    "bow_y_train = le_bow.fit_transform(bow_y_train)\n",
    "bow_y_test = le_bow.transform(bow_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_stm = preprocessing.LabelEncoder()\n",
    "STM_y_train = le_stm.fit_transform(STM_y_train)\n",
    "STM_y_test = le_stm.transform(STM_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bert = preprocessing.LabelEncoder()\n",
    "Bert_y_train = le_bert.fit_transform(Bert_y_train)\n",
    "Bert_y_test = le_bert.transform(Bert_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_stm_selected = preprocessing.LabelEncoder()\n",
    "STM_y_selected_train = le_stm.fit_transform(STM_y_selected_train)\n",
    "STM_y_selected_test = le_stm.transform(STM_y_selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bert_selected = preprocessing.LabelEncoder()\n",
    "Bert_y_selected_train = le_bert.fit_transform(Bert_y_selected_train)\n",
    "Bert_y_selected_test = le_bert.transform(Bert_y_selected_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=101, max_iter=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_LR = {\"C\": np.logspace(-4, 4, 20), \"class_weight\":[None, \"balanced\"], \"solver\": [\"liblinear\", \"lbfgs\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=0.08858667904100823, class_weight='balanced',\n",
      "                   max_iter=1000, random_state=101, solver='liblinear')\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6983480196414329\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 0.08858667904100823}\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_LR_BOW = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_BOW.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_BOW.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_BOW.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_BOW.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.710394265232975"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "LR_final_BOW = Randomized_search_LR_BOW.best_estimator_\n",
    "LR_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_LR = LR_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=206.913808111479, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6173189337214484\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'lbfgs', 'class_weight': 'balanced', 'C': 206.913808111479}\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_LR_STM = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6275478690549723"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "LR_final_STM = Randomized_search_LR_STM.best_estimator_\n",
    "LR_final_STM.fit(STM_X_train, STM_y_train)\n",
    "STM_y_pred_LR = LR_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, STM_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=11.288378916846883, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101, solver='liblinear')\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6111168855500665\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 11.288378916846883}\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_LR_STM_selected = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6275968992248062"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "LR_final_STM_selected = Randomized_search_LR_STM_selected.best_estimator_\n",
    "LR_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_LR = LR_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=78.47599703514607, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101, solver='liblinear')\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.48916438711040655\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 78.47599703514607}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_LR_Bert = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4782742681047765"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final 0.48 veel features, 0.42 outlier reduction\n",
    "LR_final_Bert = Randomized_search_LR_Bert.best_estimator_\n",
    "LR_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_LR = LR_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR2 = LogisticRegression(C=78.47599703514607, class_weight='balanced', max_iter=1000, random_state=101, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4782742681047765\n"
     ]
    }
   ],
   "source": [
    "LR2.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_LR = LR2.predict(Bert_X_test)\n",
    "#test score\n",
    "print(f1_score(Bert_y_test, Bert_y_pred_LR))\n",
    "cm = confusion_matrix(Bert_y_test, Bert_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LR2.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31472\\2776107318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdisp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\victo\\anaconda3\\lib\\site-packages\\matplotlib\\_api\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m    223\u001b[0m             f\"module {cls.__module__!r} has no attribute {name!r}\")\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'show'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LogisticRegression(C=10000.0, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101, solver='liblinear')\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.4840879301824564\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 10000.0}\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_LR_Bert_selected = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_LR_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4691191360107998"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "LR_final_Bert_selected = Randomized_search_LR_Bert_selected.best_estimator_\n",
    "LR_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_LR = LR_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_LR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Support vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = LinearSVC(random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_SVM = {\"C\": np.logspace(-4, 4, 20), \"class_weight\":[None, \"balanced\"], \"loss\": [\"hinge\", \"squared_hinge\"]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=0.23357214690901212, class_weight='balanced', random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.646745142179632\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 0.23357214690901212}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_SVM_BOW = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_BOW.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_BOW.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_BOW.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_BOW.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6433837795886689"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "SVM_final_BOW = Randomized_search_SVM_BOW.best_estimator_\n",
    "SVM_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_SVM = SVM_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=545.5594781168514, class_weight='balanced', random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.5875047527680077\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 545.5594781168514}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_SVM_STM = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5844490216271885"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "SVM_final_STM = Randomized_search_SVM_STM.best_estimator_\n",
    "SVM_final_STM.fit(STM_X_train, STM_y_train)\n",
    "STM_y_pred_SVM = SVM_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, STM_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=29.763514416313132, class_weight='balanced', loss='hinge',\n",
      "          random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6105825495065169\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'hinge', 'class_weight': 'balanced', 'C': 29.763514416313132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_SVM_STM_selected = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6305911792305287"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "SVM_final_STM_selected = Randomized_search_SVM_STM_selected.best_estimator_\n",
    "SVM_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_SVM = SVM_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=0.615848211066026, class_weight='balanced', random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.4945809432992479\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 0.615848211066026}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_SVM_Bert = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47153965785381025"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final 0.47, 0.42 reduction\n",
    "SVM_final_Bert = Randomized_search_SVM_Bert.best_estimator_\n",
    "SVM_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_SVM = SVM_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n LinearSVC(C=0.615848211066026, class_weight='balanced', loss='hinge',\n",
      "          random_state=101)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.4741281996289698\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'loss': 'hinge', 'class_weight': 'balanced', 'C': 0.615848211066026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_SVM_Bert_selected = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_SVM_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4594395280235988"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "SVM_final_Bert_selected = Randomized_search_SVM_Bert_selected.best_estimator_\n",
    "SVM_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_SVM = SVM_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(verbosity = 1, seed = 101, use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters_XGB = {\"colsample_bytree:\": np.arange(0.5,1,0.1) ,\"min_child_weight\": np.arange(1,10,1), \"eta\": np.arange(0.01,0.3,0.05), \"gamma\": np.arange(0,5,1), \"max_depth\": np.arange(3,10,1), \"subsample\": np.arange(0.5,1,0.1), \"scale_pos_weight\": [1, 4.045069258], \"objective\": [\"binary:logistic\", \"binary:logitraw\", \"binary:hinge\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[16:35:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.6,\n",
      "              enable_categorical=False, eta=0.21000000000000002, gamma=3,\n",
      "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.209999993, max_delta_step=0, max_depth=7,\n",
      "              min_child_weight=9, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              predictor='auto', random_state=101, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=4.045069258, seed=101, subsample=0.6,\n",
      "              tree_method='exact', use_label_encoder=False, ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6609908243668589\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.6, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 9, 'max_depth': 7, 'gamma': 3, 'eta': 0.21000000000000002, 'colsample_bytree:': 0.6}\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_XGB = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6636925188743994"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "XGB_final_BOW = Randomized_search_XGB.best_estimator_\n",
    "XGB_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_XGB = XGB_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[16:42:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:42:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.7,\n",
      "              enable_categorical=False, eta=0.11, gamma=1, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.109999999, max_delta_step=0, max_depth=4,\n",
      "              min_child_weight=9, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              predictor='auto', random_state=101, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.8999999999999999, tree_method='exact',\n",
      "              use_label_encoder=False, ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6223469122061476\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.8999999999999999, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 9, 'max_depth': 4, 'gamma': 1, 'eta': 0.11, 'colsample_bytree:': 0.7}\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_XGB_STM = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:42:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.633828996282528"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "XGB_final_STM = Randomized_search_XGB_STM.best_estimator_\n",
    "XGB_final_STM.fit(STM_X_train, STM_y_train)\n",
    "XGB_y_pred_STM = XGB_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, XGB_y_pred_STM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:49:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1,\n",
      "              colsample_bytree:=0.8999999999999999, enable_categorical=False,\n",
      "              eta=0.16000000000000003, gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.159999996,\n",
      "              max_delta_step=0, max_depth=9, min_child_weight=6, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.7999999999999999, tree_method='exact',\n",
      "              use_label_encoder=False, ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.6346006898675166\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.7999999999999999, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 6, 'max_depth': 9, 'gamma': 0, 'eta': 0.16000000000000003, 'colsample_bytree:': 0.8999999999999999}\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_XGB_STM_selected = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:49:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6400304414003044"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "XGB_final_STM_selected = Randomized_search_XGB_STM_selected.best_estimator_\n",
    "XGB_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_XGB = XGB_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[16:53:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:53:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1,\n",
      "              colsample_bytree:=0.7999999999999999, enable_categorical=False,\n",
      "              eta=0.26, gamma=1, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.25999999,\n",
      "              max_delta_step=0, max_depth=4, min_child_weight=2, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
      "              num_parallel_tree=1, objective='binary:logitraw',\n",
      "              predictor='auto', random_state=101, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.8999999999999999, tree_method='exact', ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.5058165558969346\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.8999999999999999, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logitraw', 'min_child_weight': 2, 'max_depth': 4, 'gamma': 1, 'eta': 0.26, 'colsample_bytree:': 0.7999999999999999}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_XGB_Bert = RandomizedSearchCV(XGB, parameters_XGB, verbose=2, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:53:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:53:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5012185215272137"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final\n",
    "XGB_final_Bert = Randomized_search_XGB_Bert.best_estimator_\n",
    "XGB_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_XGB = XGB_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB2 = XGBClassifier(verbosity = 1, seed = 101, use_label_encoder=False, subsample =  0.8999999999999999, scale_pos_weight = 4.045069258, objective =  'binary:logitraw', min_child_weight = 2, max_depth = 4, gamma = 1, eta = 0.26, colsample_bytree = 0.7999999999999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:33:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4991883116883117\n"
     ]
    }
   ],
   "source": [
    "#BERT final conf\n",
    "XGB2.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_XGB_con = XGB2.predict(Bert_X_test)\n",
    "#test score\n",
    "print(f1_score(Bert_y_test, Bert_y_pred_XGB_con))\n",
    "cm = confusion_matrix(Bert_y_test, Bert_y_pred_XGB_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_XGB = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=XGB2.classes_,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x27718a4d520>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp_XGB.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:57:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:57:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\\\\n The best estimator across ALL searched params:\\\\n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.6,\n",
      "              enable_categorical=False, eta=0.11, gamma=4, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.109999999, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='binary:logitraw', predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.7, tree_method='exact', ...)\n",
      "\\\\n The best score across ALL searched params:\\\\n 0.4938053531853967\n",
      "\\\\n The best parameters across ALL searched params:\\\\n {'subsample': 0.7, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logitraw', 'min_child_weight': 4, 'max_depth': 6, 'gamma': 4, 'eta': 0.11, 'colsample_bytree:': 0.6}\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_XGB_Bert_selected = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\\\\\n The best estimator across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert_selected.best_estimator_)\n",
    "print(\"\\\\\\\\n The best score across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert_selected.best_score_)\n",
    "print(\"\\\\\\\\n The best parameters across ALL searched params:\\\\\\\\n\",Randomized_search_XGB_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:57:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:57:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4970513900589722"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "XGB_final_Bert_selected = Randomized_search_XGB_Bert_selected.best_estimator_\n",
    "XGB_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_XGB = XGB_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_XGB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
