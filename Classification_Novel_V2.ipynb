{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter searching: https://www.projectpro.io/recipes/find-optimal-parameters-using-gridsearchcv \\\n",
    "Renaming the last column: https://stackoverflow.com/questions/56479835/rename-only-the-last-column-in-pandas-dataframe-accounting-for-duplicate-header"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in dataset normal, STM and BerTopic\n",
    "df = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Visual Novel\\\\novel_final.csv\")\n",
    "df_STM = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Visual Novel\\\\features_novel_STM.csv\")\n",
    "df_Bert = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Visual Novel\\\\features_novel_Bert_reduction.csv\")\n",
    "df_STM_selected = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Visual Novel\\\\Novel_featured_selected_STM.csv\")\n",
    "df_Bert_selected = pandas.read_csv(\"G:\\\\Master\\\\Block 3\\\\Thesis\\\\Visual Novel\\\\Novel_featured_selected_Bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_STM.columns = [*df_STM.columns[:-1], 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V132</th>\n",
       "      <th>V133</th>\n",
       "      <th>V134</th>\n",
       "      <th>V135</th>\n",
       "      <th>V136</th>\n",
       "      <th>V137</th>\n",
       "      <th>V138</th>\n",
       "      <th>V139</th>\n",
       "      <th>V140</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010644</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.005637</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015769</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.006606</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015655</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.009651</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24393</th>\n",
       "      <td>0.013314</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24394</th>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>0.015171</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.019785</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.015730</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24396</th>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24397</th>\n",
       "      <td>0.003967</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24398 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      0.007000  0.000534  0.000154  0.002667  0.000652  0.000063  0.002113   \n",
       "1      0.010644  0.002984  0.000329  0.005637  0.001644  0.000152  0.002915   \n",
       "2      0.015769  0.002893  0.000479  0.006606  0.002713  0.000180  0.003004   \n",
       "3      0.015655  0.002680  0.000505  0.004093  0.001991  0.000202  0.003575   \n",
       "4      0.010775  0.001805  0.000253  0.001813  0.000964  0.000127  0.001374   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24393  0.013314  0.001260  0.000875  0.016534  0.001928  0.000262  0.009763   \n",
       "24394  0.011417  0.002197  0.000461  0.002321  0.000832  0.000099  0.005855   \n",
       "24395  0.015171  0.002369  0.001274  0.006574  0.019785  0.000477  0.015730   \n",
       "24396  0.005745  0.001693  0.001037  0.002862  0.002556  0.000261  0.002940   \n",
       "24397  0.003967  0.001718  0.000531  0.002387  0.001538  0.000169  0.007716   \n",
       "\n",
       "             V8        V9       V10  ...      V132      V133      V134  \\\n",
       "0      0.000080  0.000242  0.000081  ...  0.000209  0.000495  0.000071   \n",
       "1      0.000395  0.000397  0.000422  ...  0.000661  0.003246  0.000197   \n",
       "2      0.000808  0.000284  0.000342  ...  0.000348  0.001685  0.000322   \n",
       "3      0.000999  0.000404  0.000613  ...  0.000525  0.000830  0.000548   \n",
       "4      0.000900  0.000169  0.000936  ...  0.000536  0.001483  0.000685   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24393  0.001318  0.001011  0.001031  ...  0.000723  0.000392  0.034757   \n",
       "24394  0.000412  0.000285  0.000660  ...  0.001373  0.002943  0.000750   \n",
       "24395  0.001467  0.004890  0.000823  ...  0.001075  0.001369  0.000582   \n",
       "24396  0.003071  0.000351  0.001119  ...  0.000813  0.001951  0.001178   \n",
       "24397  0.000949  0.000179  0.001464  ...  0.001327  0.003082  0.002188   \n",
       "\n",
       "           V135      V136      V137      V138      V139      V140  sentiment  \n",
       "0      0.000106  0.007973  0.000108  0.000894  0.000280  0.000501          0  \n",
       "1      0.000400  0.025340  0.000795  0.002710  0.002697  0.001717          0  \n",
       "2      0.000742  0.009953  0.000454  0.002182  0.003498  0.001445          0  \n",
       "3      0.000810  0.017556  0.000948  0.002389  0.001240  0.001797          0  \n",
       "4      0.000512  0.009651  0.000751  0.001871  0.000290  0.001706          0  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "24393  0.000498  0.008095  0.001314  0.003711  0.000701  0.002690          0  \n",
       "24394  0.000227  0.004817  0.000677  0.001135  0.001971  0.001646          0  \n",
       "24395  0.000668  0.008259  0.001191  0.002114  0.001990  0.002499          0  \n",
       "24396  0.000453  0.004981  0.001410  0.001626  0.002465  0.002457          0  \n",
       "24397  0.000535  0.005612  0.001241  0.001682  0.001681  0.002475          0  \n",
       "\n",
       "[24398 rows x 141 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_STM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.161076e-02</td>\n",
       "      <td>8.032540e-03</td>\n",
       "      <td>3.878027e-02</td>\n",
       "      <td>5.338093e-02</td>\n",
       "      <td>9.668135e-03</td>\n",
       "      <td>1.103647e-01</td>\n",
       "      <td>9.359265e-03</td>\n",
       "      <td>4.005914e-03</td>\n",
       "      <td>2.205113e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.481152e-03</td>\n",
       "      <td>6.428300e-04</td>\n",
       "      <td>2.199288e-03</td>\n",
       "      <td>6.305197e-04</td>\n",
       "      <td>2.767314e-03</td>\n",
       "      <td>1.181205e-03</td>\n",
       "      <td>1.606444e-03</td>\n",
       "      <td>5.088521e-03</td>\n",
       "      <td>2.379915e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.493316e-305</td>\n",
       "      <td>9.494455e-307</td>\n",
       "      <td>2.652067e-306</td>\n",
       "      <td>1.078227e-305</td>\n",
       "      <td>5.150457e-307</td>\n",
       "      <td>2.696512e-306</td>\n",
       "      <td>1.653094e-306</td>\n",
       "      <td>7.652599e-307</td>\n",
       "      <td>2.836072e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>7.218034e-307</td>\n",
       "      <td>6.955792e-308</td>\n",
       "      <td>4.692486e-307</td>\n",
       "      <td>6.442587e-308</td>\n",
       "      <td>9.064081e-307</td>\n",
       "      <td>1.632524e-307</td>\n",
       "      <td>2.493031e-307</td>\n",
       "      <td>1.166737e-306</td>\n",
       "      <td>6.489213e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.049841e-02</td>\n",
       "      <td>1.086049e-02</td>\n",
       "      <td>4.392182e-02</td>\n",
       "      <td>7.037843e-02</td>\n",
       "      <td>1.118475e-02</td>\n",
       "      <td>1.439517e-01</td>\n",
       "      <td>1.241098e-02</td>\n",
       "      <td>5.032778e-03</td>\n",
       "      <td>2.956815e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.129165e-03</td>\n",
       "      <td>8.568429e-04</td>\n",
       "      <td>2.803146e-03</td>\n",
       "      <td>8.426022e-04</td>\n",
       "      <td>3.448951e-03</td>\n",
       "      <td>1.566979e-03</td>\n",
       "      <td>2.173763e-03</td>\n",
       "      <td>6.936084e-03</td>\n",
       "      <td>3.002028e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.427132e-02</td>\n",
       "      <td>2.668340e-03</td>\n",
       "      <td>8.253728e-03</td>\n",
       "      <td>3.470797e-02</td>\n",
       "      <td>1.473610e-03</td>\n",
       "      <td>1.263733e-02</td>\n",
       "      <td>4.589480e-03</td>\n",
       "      <td>1.888938e-03</td>\n",
       "      <td>7.714401e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.670334e-03</td>\n",
       "      <td>1.870692e-04</td>\n",
       "      <td>1.296126e-03</td>\n",
       "      <td>1.756435e-04</td>\n",
       "      <td>1.876603e-03</td>\n",
       "      <td>4.163106e-04</td>\n",
       "      <td>6.744426e-04</td>\n",
       "      <td>8.706816e-03</td>\n",
       "      <td>1.536956e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.557301e-305</td>\n",
       "      <td>1.623180e-306</td>\n",
       "      <td>1.016503e-306</td>\n",
       "      <td>6.989701e-306</td>\n",
       "      <td>2.686663e-307</td>\n",
       "      <td>8.231256e-307</td>\n",
       "      <td>2.393766e-306</td>\n",
       "      <td>5.173294e-307</td>\n",
       "      <td>5.666313e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>6.207140e-307</td>\n",
       "      <td>9.166659e-308</td>\n",
       "      <td>2.013440e-306</td>\n",
       "      <td>7.912663e-308</td>\n",
       "      <td>3.934850e-307</td>\n",
       "      <td>2.630371e-307</td>\n",
       "      <td>1.948564e-306</td>\n",
       "      <td>2.720192e-307</td>\n",
       "      <td>7.043960e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24414</th>\n",
       "      <td>24414</td>\n",
       "      <td>5.892645e-02</td>\n",
       "      <td>3.810099e-01</td>\n",
       "      <td>7.971860e-03</td>\n",
       "      <td>4.219291e-02</td>\n",
       "      <td>2.325838e-03</td>\n",
       "      <td>6.281696e-03</td>\n",
       "      <td>1.334045e-02</td>\n",
       "      <td>3.140196e-03</td>\n",
       "      <td>1.315501e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.226765e-03</td>\n",
       "      <td>1.638025e-03</td>\n",
       "      <td>3.035198e-03</td>\n",
       "      <td>1.409934e-03</td>\n",
       "      <td>1.879297e-03</td>\n",
       "      <td>2.122318e-03</td>\n",
       "      <td>3.961188e-03</td>\n",
       "      <td>1.872837e-03</td>\n",
       "      <td>2.317176e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24415</th>\n",
       "      <td>24415</td>\n",
       "      <td>1.332325e-305</td>\n",
       "      <td>1.174029e-306</td>\n",
       "      <td>1.536448e-306</td>\n",
       "      <td>6.278282e-305</td>\n",
       "      <td>3.829572e-307</td>\n",
       "      <td>2.006488e-306</td>\n",
       "      <td>2.036811e-306</td>\n",
       "      <td>4.497688e-307</td>\n",
       "      <td>3.066523e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>4.350305e-307</td>\n",
       "      <td>7.493255e-308</td>\n",
       "      <td>4.584003e-307</td>\n",
       "      <td>7.223216e-308</td>\n",
       "      <td>3.865857e-307</td>\n",
       "      <td>1.467236e-307</td>\n",
       "      <td>2.824223e-307</td>\n",
       "      <td>8.797701e-307</td>\n",
       "      <td>4.237971e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24416</th>\n",
       "      <td>24416</td>\n",
       "      <td>7.422404e-305</td>\n",
       "      <td>1.256117e-306</td>\n",
       "      <td>1.412481e-306</td>\n",
       "      <td>1.247427e-305</td>\n",
       "      <td>3.420711e-307</td>\n",
       "      <td>1.285612e-306</td>\n",
       "      <td>4.951853e-306</td>\n",
       "      <td>6.800412e-307</td>\n",
       "      <td>3.894692e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>1.348433e-306</td>\n",
       "      <td>8.032551e-308</td>\n",
       "      <td>1.327652e-306</td>\n",
       "      <td>7.235960e-308</td>\n",
       "      <td>7.230003e-307</td>\n",
       "      <td>2.089216e-307</td>\n",
       "      <td>4.765983e-307</td>\n",
       "      <td>4.826088e-307</td>\n",
       "      <td>1.413702e-306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24417</th>\n",
       "      <td>24417</td>\n",
       "      <td>1.963084e-01</td>\n",
       "      <td>4.939740e-03</td>\n",
       "      <td>9.086108e-03</td>\n",
       "      <td>5.982562e-02</td>\n",
       "      <td>2.009785e-03</td>\n",
       "      <td>9.245230e-03</td>\n",
       "      <td>1.011308e-02</td>\n",
       "      <td>3.464405e-03</td>\n",
       "      <td>1.482295e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>4.694588e-03</td>\n",
       "      <td>3.437601e-04</td>\n",
       "      <td>2.754826e-03</td>\n",
       "      <td>3.160989e-04</td>\n",
       "      <td>4.247605e-03</td>\n",
       "      <td>8.322930e-04</td>\n",
       "      <td>1.429033e-03</td>\n",
       "      <td>4.186135e-03</td>\n",
       "      <td>4.152724e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24418</th>\n",
       "      <td>24418</td>\n",
       "      <td>1.480489e-01</td>\n",
       "      <td>4.779244e-03</td>\n",
       "      <td>8.656063e-03</td>\n",
       "      <td>7.317343e-02</td>\n",
       "      <td>1.902357e-03</td>\n",
       "      <td>9.971060e-03</td>\n",
       "      <td>1.042400e-02</td>\n",
       "      <td>3.097521e-03</td>\n",
       "      <td>1.398816e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.871339e-03</td>\n",
       "      <td>3.271471e-04</td>\n",
       "      <td>2.701431e-03</td>\n",
       "      <td>3.034504e-04</td>\n",
       "      <td>3.483970e-03</td>\n",
       "      <td>7.590788e-04</td>\n",
       "      <td>1.349473e-03</td>\n",
       "      <td>5.021864e-03</td>\n",
       "      <td>3.525269e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24419 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0              0              1              2              3  \\\n",
       "0               0   7.161076e-02   8.032540e-03   3.878027e-02   5.338093e-02   \n",
       "1               1  2.493316e-305  9.494455e-307  2.652067e-306  1.078227e-305   \n",
       "2               2   9.049841e-02   1.086049e-02   4.392182e-02   7.037843e-02   \n",
       "3               3   5.427132e-02   2.668340e-03   8.253728e-03   3.470797e-02   \n",
       "4               4  1.557301e-305  1.623180e-306  1.016503e-306  6.989701e-306   \n",
       "...           ...            ...            ...            ...            ...   \n",
       "24414       24414   5.892645e-02   3.810099e-01   7.971860e-03   4.219291e-02   \n",
       "24415       24415  1.332325e-305  1.174029e-306  1.536448e-306  6.278282e-305   \n",
       "24416       24416  7.422404e-305  1.256117e-306  1.412481e-306  1.247427e-305   \n",
       "24417       24417   1.963084e-01   4.939740e-03   9.086108e-03   5.982562e-02   \n",
       "24418       24418   1.480489e-01   4.779244e-03   8.656063e-03   7.317343e-02   \n",
       "\n",
       "                   4              5              6              7  \\\n",
       "0       9.668135e-03   1.103647e-01   9.359265e-03   4.005914e-03   \n",
       "1      5.150457e-307  2.696512e-306  1.653094e-306  7.652599e-307   \n",
       "2       1.118475e-02   1.439517e-01   1.241098e-02   5.032778e-03   \n",
       "3       1.473610e-03   1.263733e-02   4.589480e-03   1.888938e-03   \n",
       "4      2.686663e-307  8.231256e-307  2.393766e-306  5.173294e-307   \n",
       "...              ...            ...            ...            ...   \n",
       "24414   2.325838e-03   6.281696e-03   1.334045e-02   3.140196e-03   \n",
       "24415  3.829572e-307  2.006488e-306  2.036811e-306  4.497688e-307   \n",
       "24416  3.420711e-307  1.285612e-306  4.951853e-306  6.800412e-307   \n",
       "24417   2.009785e-03   9.245230e-03   1.011308e-02   3.464405e-03   \n",
       "24418   1.902357e-03   9.971060e-03   1.042400e-02   3.097521e-03   \n",
       "\n",
       "                   8  ...            131            132            133  \\\n",
       "0       2.205113e-03  ...   2.481152e-03   6.428300e-04   2.199288e-03   \n",
       "1      2.836072e-307  ...  7.218034e-307  6.955792e-308  4.692486e-307   \n",
       "2       2.956815e-03  ...   3.129165e-03   8.568429e-04   2.803146e-03   \n",
       "3       7.714401e-04  ...   1.670334e-03   1.870692e-04   1.296126e-03   \n",
       "4      5.666313e-307  ...  6.207140e-307  9.166659e-308  2.013440e-306   \n",
       "...              ...  ...            ...            ...            ...   \n",
       "24414   1.315501e-02  ...   2.226765e-03   1.638025e-03   3.035198e-03   \n",
       "24415  3.066523e-307  ...  4.350305e-307  7.493255e-308  4.584003e-307   \n",
       "24416  3.894692e-307  ...  1.348433e-306  8.032551e-308  1.327652e-306   \n",
       "24417   1.482295e-03  ...   4.694588e-03   3.437601e-04   2.754826e-03   \n",
       "24418   1.398816e-03  ...   3.871339e-03   3.271471e-04   2.701431e-03   \n",
       "\n",
       "                 134            135            136            137  \\\n",
       "0       6.305197e-04   2.767314e-03   1.181205e-03   1.606444e-03   \n",
       "1      6.442587e-308  9.064081e-307  1.632524e-307  2.493031e-307   \n",
       "2       8.426022e-04   3.448951e-03   1.566979e-03   2.173763e-03   \n",
       "3       1.756435e-04   1.876603e-03   4.163106e-04   6.744426e-04   \n",
       "4      7.912663e-308  3.934850e-307  2.630371e-307  1.948564e-306   \n",
       "...              ...            ...            ...            ...   \n",
       "24414   1.409934e-03   1.879297e-03   2.122318e-03   3.961188e-03   \n",
       "24415  7.223216e-308  3.865857e-307  1.467236e-307  2.824223e-307   \n",
       "24416  7.235960e-308  7.230003e-307  2.089216e-307  4.765983e-307   \n",
       "24417   3.160989e-04   4.247605e-03   8.322930e-04   1.429033e-03   \n",
       "24418   3.034504e-04   3.483970e-03   7.590788e-04   1.349473e-03   \n",
       "\n",
       "                 138            139  sentiment  \n",
       "0       5.088521e-03   2.379915e-03          0  \n",
       "1      1.166737e-306  6.489213e-307          0  \n",
       "2       6.936084e-03   3.002028e-03          0  \n",
       "3       8.706816e-03   1.536956e-03          0  \n",
       "4      2.720192e-307  7.043960e-307          0  \n",
       "...              ...            ...        ...  \n",
       "24414   1.872837e-03   2.317176e-03          0  \n",
       "24415  8.797701e-307  4.237971e-307          0  \n",
       "24416  4.826088e-307  1.413702e-306          0  \n",
       "24417   4.186135e-03   4.152724e-03          0  \n",
       "24418   5.021864e-03   3.525269e-03          0  \n",
       "\n",
       "[24419 rows x 142 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V110</th>\n",
       "      <th>V112</th>\n",
       "      <th>V114</th>\n",
       "      <th>V115</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V118</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.013634</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.014413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004670</td>\n",
       "      <td>0.024581</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.028220</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.038371</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.022293</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>0.014445</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.006629</td>\n",
       "      <td>0.013418</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.008105</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.021453</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006103</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.024977</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.028230</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.019814</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23705</th>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.025920</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.632710</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.019811</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.019861</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23706</th>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.103910</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.152154</td>\n",
       "      <td>0.062349</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.039036</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.007715</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23707</th>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.041023</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.389163</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.048832</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23708</th>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.349711</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.040336</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.020990</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23709</th>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.009245</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.716765</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.034050</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23710 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      0.009384  0.000191  0.009398  0.000351  0.000357  0.000684  0.008610   \n",
       "1      0.000932  0.000472  0.014335  0.000482  0.000690  0.001036  0.031984   \n",
       "2      0.001011  0.000522  0.003377  0.000476  0.000583  0.001198  0.038371   \n",
       "3      0.001061  0.000450  0.002850  0.000470  0.000677  0.006629  0.013418   \n",
       "4      0.006103  0.001262  0.002408  0.000701  0.000492  0.001091  0.024977   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23705  0.000545  0.000989  0.001783  0.000576  0.000500  0.000824  0.025920   \n",
       "23706  0.001119  0.004776  0.004084  0.000533  0.000882  0.001457  0.103910   \n",
       "23707  0.001127  0.001692  0.002784  0.000591  0.000650  0.001259  0.041023   \n",
       "23708  0.005860  0.002915  0.006412  0.001119  0.000993  0.001276  0.013736   \n",
       "23709  0.002068  0.000481  0.001326  0.000195  0.000413  0.000577  0.009245   \n",
       "\n",
       "             V8        V9       V10  ...      V110      V112      V114  \\\n",
       "0      0.000363  0.001423  0.001404  ...  0.002492  0.003593  0.000549   \n",
       "1      0.000431  0.004122  0.014413  ...  0.004670  0.024581  0.001397   \n",
       "2      0.000513  0.000748  0.012403  ...  0.001619  0.022293  0.001858   \n",
       "3      0.000427  0.000741  0.002091  ...  0.002858  0.008105  0.015504   \n",
       "4      0.000474  0.000587  0.003748  ...  0.003220  0.003269  0.000993   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23705  0.000547  0.000491  0.001797  ...  0.000613  0.001061  0.632710   \n",
       "23706  0.000865  0.000838  0.005135  ...  0.001910  0.001119  0.152154   \n",
       "23707  0.000654  0.000457  0.004366  ...  0.002633  0.002555  0.389163   \n",
       "23708  0.000538  0.000233  0.001824  ...  0.000925  0.002049  0.349711   \n",
       "23709  0.000196  0.000242  0.001130  ...  0.000233  0.000700  0.716765   \n",
       "\n",
       "           V115      V116      V117      V118      V119      V120  sentiment  \n",
       "0      0.001936  0.006400  0.009484  0.002204  0.013634  0.001648          0  \n",
       "1      0.002799  0.001505  0.028220  0.001497  0.016129  0.002868          0  \n",
       "2      0.011528  0.014445  0.078358  0.001269  0.017963  0.002458          0  \n",
       "3      0.006458  0.002311  0.016802  0.002514  0.021453  0.002285          0  \n",
       "4      0.028230  0.015124  0.019814  0.001969  0.024759  0.002195          0  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "23705  0.003726  0.001658  0.019811  0.000489  0.019861  0.001216          0  \n",
       "23706  0.062349  0.000939  0.039036  0.000536  0.007715  0.002403          0  \n",
       "23707  0.009444  0.002265  0.048832  0.000740  0.016431  0.002152          0  \n",
       "23708  0.003704  0.001549  0.040336  0.001165  0.020990  0.001708          0  \n",
       "23709  0.002021  0.000602  0.034050  0.000364  0.004626  0.000733          0  \n",
       "\n",
       "[23710 rows x 107 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_STM_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.142453e-306</td>\n",
       "      <td>4.507935e-306</td>\n",
       "      <td>2.813306e-307</td>\n",
       "      <td>4.944254e-306</td>\n",
       "      <td>1.399707e-306</td>\n",
       "      <td>4.850360e-307</td>\n",
       "      <td>1.888014e-306</td>\n",
       "      <td>2.142736e-307</td>\n",
       "      <td>1.230177e-306</td>\n",
       "      <td>4.063005e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>1.921869e-307</td>\n",
       "      <td>1.487461e-307</td>\n",
       "      <td>7.081745e-307</td>\n",
       "      <td>1.002271e-307</td>\n",
       "      <td>1.380318e-307</td>\n",
       "      <td>7.178252e-308</td>\n",
       "      <td>1.344925e-307</td>\n",
       "      <td>5.447303e-307</td>\n",
       "      <td>3.946912e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.784490e-02</td>\n",
       "      <td>7.095329e-02</td>\n",
       "      <td>1.570930e-03</td>\n",
       "      <td>2.376464e-01</td>\n",
       "      <td>4.843239e-03</td>\n",
       "      <td>3.847580e-03</td>\n",
       "      <td>6.702729e-03</td>\n",
       "      <td>1.038302e-03</td>\n",
       "      <td>1.293865e-02</td>\n",
       "      <td>2.181121e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.015032e-03</td>\n",
       "      <td>4.573897e-04</td>\n",
       "      <td>2.496544e-03</td>\n",
       "      <td>4.882725e-04</td>\n",
       "      <td>8.963486e-04</td>\n",
       "      <td>3.735314e-04</td>\n",
       "      <td>6.839792e-04</td>\n",
       "      <td>8.813099e-03</td>\n",
       "      <td>1.687375e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.009378e-02</td>\n",
       "      <td>5.740819e-02</td>\n",
       "      <td>1.845591e-03</td>\n",
       "      <td>1.026023e-01</td>\n",
       "      <td>7.553949e-03</td>\n",
       "      <td>4.672788e-03</td>\n",
       "      <td>2.803940e-02</td>\n",
       "      <td>1.202401e-03</td>\n",
       "      <td>1.230041e-02</td>\n",
       "      <td>6.043041e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053780e-03</td>\n",
       "      <td>7.444689e-04</td>\n",
       "      <td>3.930355e-02</td>\n",
       "      <td>7.251219e-04</td>\n",
       "      <td>1.048219e-03</td>\n",
       "      <td>4.820143e-04</td>\n",
       "      <td>1.051534e-03</td>\n",
       "      <td>7.001478e-03</td>\n",
       "      <td>7.615695e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.153328e-306</td>\n",
       "      <td>4.619833e-306</td>\n",
       "      <td>2.833306e-307</td>\n",
       "      <td>5.152505e-306</td>\n",
       "      <td>1.371597e-306</td>\n",
       "      <td>4.926420e-307</td>\n",
       "      <td>1.945997e-306</td>\n",
       "      <td>2.137728e-307</td>\n",
       "      <td>1.248045e-306</td>\n",
       "      <td>4.167725e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>1.953384e-307</td>\n",
       "      <td>1.470140e-307</td>\n",
       "      <td>7.663814e-307</td>\n",
       "      <td>1.012614e-307</td>\n",
       "      <td>1.400051e-307</td>\n",
       "      <td>7.230914e-308</td>\n",
       "      <td>1.363777e-307</td>\n",
       "      <td>5.718621e-307</td>\n",
       "      <td>4.098344e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.846680e-02</td>\n",
       "      <td>5.984713e-02</td>\n",
       "      <td>1.243980e-03</td>\n",
       "      <td>5.759698e-02</td>\n",
       "      <td>4.047451e-03</td>\n",
       "      <td>3.111914e-03</td>\n",
       "      <td>4.703065e-03</td>\n",
       "      <td>8.789245e-04</td>\n",
       "      <td>1.377866e-02</td>\n",
       "      <td>1.523865e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.991246e-03</td>\n",
       "      <td>3.351130e-04</td>\n",
       "      <td>1.597087e-03</td>\n",
       "      <td>3.561149e-04</td>\n",
       "      <td>6.848492e-04</td>\n",
       "      <td>2.855930e-04</td>\n",
       "      <td>4.893787e-04</td>\n",
       "      <td>4.710055e-03</td>\n",
       "      <td>1.146230e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23724</th>\n",
       "      <td>9.274574e-307</td>\n",
       "      <td>2.826757e-306</td>\n",
       "      <td>3.075184e-307</td>\n",
       "      <td>1.948046e-306</td>\n",
       "      <td>4.016175e-307</td>\n",
       "      <td>8.730075e-307</td>\n",
       "      <td>7.418250e-307</td>\n",
       "      <td>8.827305e-308</td>\n",
       "      <td>7.512172e-307</td>\n",
       "      <td>5.104187e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>1.407230e-307</td>\n",
       "      <td>7.307047e-308</td>\n",
       "      <td>1.376313e-307</td>\n",
       "      <td>6.771631e-307</td>\n",
       "      <td>2.010246e-307</td>\n",
       "      <td>1.181646e-307</td>\n",
       "      <td>6.422830e-307</td>\n",
       "      <td>1.316008e-307</td>\n",
       "      <td>1.608513e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23725</th>\n",
       "      <td>3.649919e-02</td>\n",
       "      <td>7.017465e-02</td>\n",
       "      <td>1.983904e-03</td>\n",
       "      <td>5.551481e-02</td>\n",
       "      <td>6.578053e-03</td>\n",
       "      <td>4.255166e-03</td>\n",
       "      <td>6.474361e-03</td>\n",
       "      <td>1.559232e-03</td>\n",
       "      <td>2.144182e-02</td>\n",
       "      <td>2.081870e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.866680e-03</td>\n",
       "      <td>5.187639e-04</td>\n",
       "      <td>2.109422e-03</td>\n",
       "      <td>5.337382e-04</td>\n",
       "      <td>1.031448e-03</td>\n",
       "      <td>4.433468e-04</td>\n",
       "      <td>7.201342e-04</td>\n",
       "      <td>5.033651e-03</td>\n",
       "      <td>1.535652e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23726</th>\n",
       "      <td>1.880700e-305</td>\n",
       "      <td>1.268147e-305</td>\n",
       "      <td>3.769829e-307</td>\n",
       "      <td>1.102243e-305</td>\n",
       "      <td>1.512960e-306</td>\n",
       "      <td>8.145502e-307</td>\n",
       "      <td>1.379821e-306</td>\n",
       "      <td>3.309206e-307</td>\n",
       "      <td>3.985883e-306</td>\n",
       "      <td>4.226241e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>5.388096e-307</td>\n",
       "      <td>1.058979e-307</td>\n",
       "      <td>4.609925e-307</td>\n",
       "      <td>1.028694e-307</td>\n",
       "      <td>1.897654e-307</td>\n",
       "      <td>8.492358e-308</td>\n",
       "      <td>1.379965e-307</td>\n",
       "      <td>1.190897e-306</td>\n",
       "      <td>3.224572e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23727</th>\n",
       "      <td>4.010009e-03</td>\n",
       "      <td>1.345770e-02</td>\n",
       "      <td>1.362567e-03</td>\n",
       "      <td>9.214488e-03</td>\n",
       "      <td>1.823169e-03</td>\n",
       "      <td>3.730164e-03</td>\n",
       "      <td>3.441402e-03</td>\n",
       "      <td>3.986513e-04</td>\n",
       "      <td>3.479294e-03</td>\n",
       "      <td>2.176610e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.583681e-04</td>\n",
       "      <td>3.211093e-04</td>\n",
       "      <td>6.414988e-04</td>\n",
       "      <td>1.985915e-03</td>\n",
       "      <td>9.573935e-04</td>\n",
       "      <td>5.010322e-04</td>\n",
       "      <td>3.681735e-03</td>\n",
       "      <td>6.134136e-04</td>\n",
       "      <td>7.574879e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23728</th>\n",
       "      <td>9.293529e-307</td>\n",
       "      <td>2.875653e-306</td>\n",
       "      <td>3.096601e-307</td>\n",
       "      <td>1.980027e-306</td>\n",
       "      <td>4.058189e-307</td>\n",
       "      <td>8.746652e-307</td>\n",
       "      <td>7.521243e-307</td>\n",
       "      <td>8.912531e-308</td>\n",
       "      <td>7.612503e-307</td>\n",
       "      <td>5.114584e-307</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428100e-307</td>\n",
       "      <td>7.348883e-308</td>\n",
       "      <td>1.396112e-307</td>\n",
       "      <td>6.348496e-307</td>\n",
       "      <td>2.045985e-307</td>\n",
       "      <td>1.182208e-307</td>\n",
       "      <td>6.759969e-307</td>\n",
       "      <td>1.334846e-307</td>\n",
       "      <td>1.634258e-307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23729 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              2              4  \\\n",
       "0      3.142453e-306  4.507935e-306  2.813306e-307  4.944254e-306   \n",
       "1       1.784490e-02   7.095329e-02   1.570930e-03   2.376464e-01   \n",
       "2       2.009378e-02   5.740819e-02   1.845591e-03   1.026023e-01   \n",
       "3      3.153328e-306  4.619833e-306  2.833306e-307  5.152505e-306   \n",
       "4       1.846680e-02   5.984713e-02   1.243980e-03   5.759698e-02   \n",
       "...              ...            ...            ...            ...   \n",
       "23724  9.274574e-307  2.826757e-306  3.075184e-307  1.948046e-306   \n",
       "23725   3.649919e-02   7.017465e-02   1.983904e-03   5.551481e-02   \n",
       "23726  1.880700e-305  1.268147e-305  3.769829e-307  1.102243e-305   \n",
       "23727   4.010009e-03   1.345770e-02   1.362567e-03   9.214488e-03   \n",
       "23728  9.293529e-307  2.875653e-306  3.096601e-307  1.980027e-306   \n",
       "\n",
       "                   5              6              7              8  \\\n",
       "0      1.399707e-306  4.850360e-307  1.888014e-306  2.142736e-307   \n",
       "1       4.843239e-03   3.847580e-03   6.702729e-03   1.038302e-03   \n",
       "2       7.553949e-03   4.672788e-03   2.803940e-02   1.202401e-03   \n",
       "3      1.371597e-306  4.926420e-307  1.945997e-306  2.137728e-307   \n",
       "4       4.047451e-03   3.111914e-03   4.703065e-03   8.789245e-04   \n",
       "...              ...            ...            ...            ...   \n",
       "23724  4.016175e-307  8.730075e-307  7.418250e-307  8.827305e-308   \n",
       "23725   6.578053e-03   4.255166e-03   6.474361e-03   1.559232e-03   \n",
       "23726  1.512960e-306  8.145502e-307  1.379821e-306  3.309206e-307   \n",
       "23727   1.823169e-03   3.730164e-03   3.441402e-03   3.986513e-04   \n",
       "23728  4.058189e-307  8.746652e-307  7.521243e-307  8.912531e-308   \n",
       "\n",
       "                   9             11  ...            111            112  \\\n",
       "0      1.230177e-306  4.063005e-307  ...  1.921869e-307  1.487461e-307   \n",
       "1       1.293865e-02   2.181121e-03  ...   2.015032e-03   4.573897e-04   \n",
       "2       1.230041e-02   6.043041e-03  ...   2.053780e-03   7.444689e-04   \n",
       "3      1.248045e-306  4.167725e-307  ...  1.953384e-307  1.470140e-307   \n",
       "4       1.377866e-02   1.523865e-03  ...   1.991246e-03   3.351130e-04   \n",
       "...              ...            ...  ...            ...            ...   \n",
       "23724  7.512172e-307  5.104187e-307  ...  1.407230e-307  7.307047e-308   \n",
       "23725   2.144182e-02   2.081870e-03  ...   2.866680e-03   5.187639e-04   \n",
       "23726  3.985883e-306  4.226241e-307  ...  5.388096e-307  1.058979e-307   \n",
       "23727   3.479294e-03   2.176610e-03  ...   6.583681e-04   3.211093e-04   \n",
       "23728  7.612503e-307  5.114584e-307  ...  1.428100e-307  7.348883e-308   \n",
       "\n",
       "                 113            114            115            116  \\\n",
       "0      7.081745e-307  1.002271e-307  1.380318e-307  7.178252e-308   \n",
       "1       2.496544e-03   4.882725e-04   8.963486e-04   3.735314e-04   \n",
       "2       3.930355e-02   7.251219e-04   1.048219e-03   4.820143e-04   \n",
       "3      7.663814e-307  1.012614e-307  1.400051e-307  7.230914e-308   \n",
       "4       1.597087e-03   3.561149e-04   6.848492e-04   2.855930e-04   \n",
       "...              ...            ...            ...            ...   \n",
       "23724  1.376313e-307  6.771631e-307  2.010246e-307  1.181646e-307   \n",
       "23725   2.109422e-03   5.337382e-04   1.031448e-03   4.433468e-04   \n",
       "23726  4.609925e-307  1.028694e-307  1.897654e-307  8.492358e-308   \n",
       "23727   6.414988e-04   1.985915e-03   9.573935e-04   5.010322e-04   \n",
       "23728  1.396112e-307  6.348496e-307  2.045985e-307  1.182208e-307   \n",
       "\n",
       "                 117            118            119  sentiment  \n",
       "0      1.344925e-307  5.447303e-307  3.946912e-307          0  \n",
       "1       6.839792e-04   8.813099e-03   1.687375e-03          0  \n",
       "2       1.051534e-03   7.001478e-03   7.615695e-03          0  \n",
       "3      1.363777e-307  5.718621e-307  4.098344e-307          0  \n",
       "4       4.893787e-04   4.710055e-03   1.146230e-03          0  \n",
       "...              ...            ...            ...        ...  \n",
       "23724  6.422830e-307  1.316008e-307  1.608513e-307          0  \n",
       "23725   7.201342e-04   5.033651e-03   1.535652e-03          0  \n",
       "23726  1.379965e-307  1.190897e-306  3.224572e-307          0  \n",
       "23727   3.681735e-03   6.134136e-04   7.574879e-04          0  \n",
       "23728  6.759969e-307  1.334846e-307  1.634258e-307          0  \n",
       "\n",
       "[23729 rows x 106 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Bert_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the bag of words\n",
    "vectorizer_model = CountVectorizer(min_df = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the columns in X and y\n",
    "bow_X = vectorizer_model.fit_transform(df[\"review_text\"].values)\n",
    "bow_y = df[\"review_score\"].values\n",
    "STM_X = df_STM.drop(\"sentiment\", axis=1)\n",
    "STM_X = STM_X.values\n",
    "STM_y = df_STM[\"sentiment\"].values\n",
    "STM_X_selected = df_STM_selected.drop(\"sentiment\", axis = 1)\n",
    "STM_y_selected = df_STM_selected[\"sentiment\"].values\n",
    "Bert_X = df_Bert.drop([\"sentiment\", \"Unnamed: 0\"], axis=1)\n",
    "Bert_X = Bert_X.values\n",
    "Bert_y = df_Bert[\"sentiment\"].values\n",
    "Bert_X_selected = df_Bert_selected.drop(\"sentiment\", axis = 1)\n",
    "Bert_y_selected = df_Bert_selected[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24403x13703 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1871985 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the sets\n",
    "bow_X_train, bow_X_test, bow_y_train, bow_y_test = train_test_split(bow_X, bow_y, random_state = 101)\n",
    "STM_X_train, STM_X_test, STM_y_train, STM_y_test = train_test_split(STM_X, STM_y, random_state = 101)\n",
    "Bert_X_train, Bert_X_test, Bert_y_train, Bert_y_test = train_test_split(Bert_X, Bert_y, random_state = 101)\n",
    "STM_X_selected_train, STM_X_selected_test, STM_y_selected_train, STM_y_selected_test = train_test_split(STM_X_selected,STM_y_selected, random_state=101)\n",
    "Bert_X_selected_train, Bert_X_selected_test, Bert_y_selected_train, Bert_y_selected_test = train_test_split(Bert_X_selected,Bert_y_selected, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bow = preprocessing.LabelEncoder()\n",
    "bow_y_train = le_bow.fit_transform(bow_y_train)\n",
    "bow_y_test = le_bow.transform(bow_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_stm = preprocessing.LabelEncoder()\n",
    "STM_y_train = le_stm.fit_transform(STM_y_train)\n",
    "STM_y_test = le_stm.transform(STM_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bert = preprocessing.LabelEncoder()\n",
    "Bert_y_train = le_bert.fit_transform(Bert_y_train)\n",
    "Bert_y_test = le_bert.transform(Bert_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_stm_selected = preprocessing.LabelEncoder()\n",
    "STM_y_selected_train = le_stm.fit_transform(STM_y_selected_train)\n",
    "STM_y_selected_test = le_stm.transform(STM_y_selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_bert_selected = preprocessing.LabelEncoder()\n",
    "Bert_y_selected_train = le_bert.fit_transform(Bert_y_selected_train)\n",
    "Bert_y_selected_test = le_bert.transform(Bert_y_selected_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=101, max_iter=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_LR = {\"C\": np.logspace(-4, 4, 20), \"class_weight\":[None, \"balanced\"], \"solver\": [\"liblinear\", \"lbfgs\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LogisticRegression(C=0.03359818286283781, class_weight='balanced',\n",
      "                   max_iter=1000, random_state=101, solver='liblinear')\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.7175302910146397\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 0.03359818286283781}\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_LR_BOW = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_BOW.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_LR_BOW.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_LR_BOW.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_LR_BOW.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.700993012136815"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "LR_final_BOW = Randomized_search_LR_BOW.best_estimator_\n",
    "LR_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_LR = LR_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LogisticRegression(C=29.763514416313132, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101, solver='liblinear')\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6291189998964299\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 29.763514416313132}\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_LR_STM = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_LR_STM.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_LR_STM.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_LR_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6317127422942483"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "LR_final_STM = Randomized_search_LR_STM.best_estimator_\n",
    "LR_final_STM.fit(STM_X_train, STM_y_train)\n",
    "STM_y_pred_LR = LR_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, STM_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LogisticRegression(C=4.281332398719396, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101, solver='liblinear')\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6268487626721144\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 4.281332398719396}\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_LR_STM_selected = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_LR_STM_selected.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_LR_STM_selected.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_LR_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629735752944922"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "LR_final_STM_selected = Randomized_search_LR_STM_selected.best_estimator_\n",
    "LR_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_LR = LR_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LogisticRegression(C=10000.0, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=101, solver='liblinear')\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.4382439418850025\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 10000.0}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_LR_Bert = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_LR_Bert.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_LR_Bert.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_LR_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4490925868963396"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final 0.48 veel features, 0.42 outlier reduction\n",
    "LR_final_Bert = Randomized_search_LR_Bert.best_estimator_\n",
    "LR_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_LR = LR_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LogisticRegression(C=0.23357214690901212, class_weight='balanced',\n",
      "                   max_iter=1000, random_state=101, solver='liblinear')\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.42066528937669184\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'solver': 'liblinear', 'class_weight': 'balanced', 'C': 0.23357214690901212}\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_LR_Bert_selected = RandomizedSearchCV(LR, parameters_LR, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_LR_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_LR_Bert_selected.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_LR_Bert_selected.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_LR_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4276315789473684"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "LR_final_Bert_selected = Randomized_search_LR_Bert_selected.best_estimator_\n",
    "LR_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_LR = LR_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_LR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Support vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = LinearSVC(random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_SVM = {\"C\": np.logspace(-4, 4, 20), \"class_weight\":[None, \"balanced\"], \"loss\": [\"hinge\", \"squared_hinge\"]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LinearSVC(C=0.004832930238571752, class_weight='balanced', loss='hinge',\n",
      "          random_state=101)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.7106302303056621\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'loss': 'hinge', 'class_weight': 'balanced', 'C': 0.004832930238571752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_SVM_BOW = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_BOW.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_SVM_BOW.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_SVM_BOW.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_SVM_BOW.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6908315565031983"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "SVM_final_BOW = Randomized_search_SVM_BOW.best_estimator_\n",
    "SVM_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_SVM = SVM_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LinearSVC(C=206.913808111479, class_weight='balanced', random_state=101)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6341113458616922\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 206.913808111479}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_SVM_STM = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_SVM_STM.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_SVM_STM.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_SVM_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6383426966292135"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "SVM_final_STM = Randomized_search_SVM_STM.best_estimator_\n",
    "SVM_final_STM.fit(STM_X_train, STM_y_train)\n",
    "STM_y_pred_SVM = SVM_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, STM_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LinearSVC(C=29.763514416313132, class_weight='balanced', loss='hinge',\n",
      "          random_state=101)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.627691887565091\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'loss': 'hinge', 'class_weight': 'balanced', 'C': 29.763514416313132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_SVM_STM_selected = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_SVM_STM_selected.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_SVM_STM_selected.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_SVM_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62709188506473"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "SVM_final_STM_selected = Randomized_search_SVM_STM_selected.best_estimator_\n",
    "SVM_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_SVM = SVM_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LinearSVC(C=0.004832930238571752, class_weight='balanced', random_state=101)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.4207373483137441\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'loss': 'squared_hinge', 'class_weight': 'balanced', 'C': 0.004832930238571752}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_SVM_Bert = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_SVM_Bert.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_SVM_Bert.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_SVM_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4221076746849943"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final 0.47, 0.42 reduction\n",
    "SVM_final_Bert = Randomized_search_SVM_Bert.best_estimator_\n",
    "SVM_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_SVM = SVM_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LinearSVC(C=206.913808111479, class_weight='balanced', loss='hinge',\n",
      "          random_state=101)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.431319396226354\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'loss': 'hinge', 'class_weight': 'balanced', 'C': 206.913808111479}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_SVM_Bert_selected = RandomizedSearchCV(SVM, parameters_SVM, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_SVM_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_SVM_Bert_selected.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_SVM_Bert_selected.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_SVM_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4410143329658214"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "SVM_final_Bert_selected = Randomized_search_SVM_Bert_selected.best_estimator_\n",
    "SVM_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_SVM = SVM_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(verbosity = 1, seed = 101, use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters_XGB = {\"colsample_bytree:\": np.arange(0.5,1,0.1) ,\"min_child_weight\": np.arange(1,10,1), \"eta\": np.arange(0.01,0.3,0.05), \"gamma\": np.arange(0,5,1), \"max_depth\": np.arange(3,10,1), \"subsample\": np.arange(0.5,1,0.1), \"scale_pos_weight\": [1, 4.045069258], \"objective\": [\"binary:logistic\", \"binary:logitraw\", \"binary:hinge\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[11:47:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:47:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1,\n",
      "              colsample_bytree:=0.8999999999999999, enable_categorical=False,\n",
      "              eta=0.26, gamma=2, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.25999999,\n",
      "              max_delta_step=0, max_depth=7, min_child_weight=2, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.6, tree_method='exact', use_label_encoder=False, ...)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6530713544278902\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'subsample': 0.6, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 2, 'max_depth': 7, 'gamma': 2, 'eta': 0.26, 'colsample_bytree:': 0.8999999999999999}\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "Randomized_search_XGB = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB.fit(bow_X_train, bow_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_XGB.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_XGB.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:47:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:47:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6791586998087955"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW final\n",
    "XGB_final_BOW = Randomized_search_XGB.best_estimator_\n",
    "XGB_final_BOW.fit(bow_X_train, bow_y_train)\n",
    "bow_y_pred_XGB = XGB_final_BOW.predict(bow_X_test)\n",
    "#test score\n",
    "f1_score(bow_y_test, bow_y_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[11:52:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:52:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.6,\n",
      "              enable_categorical=False, eta=0.01, gamma=1, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.00999999978, max_delta_step=0, max_depth=7,\n",
      "              min_child_weight=8, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='binary:logitraw', predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.7, tree_method='exact', ...)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6301862407102674\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'subsample': 0.7, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logitraw', 'min_child_weight': 8, 'max_depth': 7, 'gamma': 1, 'eta': 0.01, 'colsample_bytree:': 0.6}\n"
     ]
    }
   ],
   "source": [
    "#STM\n",
    "Randomized_search_XGB_STM = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_STM.fit(STM_X_train, STM_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_XGB_STM.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_XGB_STM.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_XGB_STM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:52:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:52:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.628482972136223"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM final\n",
    "XGB_final_STM = Randomized_search_XGB_STM.best_estimator_\n",
    "XGB_final_STM.fit(STM_X_train, STM_y_train)\n",
    "XGB_y_pred_STM = XGB_final_STM.predict(STM_X_test)\n",
    "#test score\n",
    "f1_score(STM_y_test, XGB_y_pred_STM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:57:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:57:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.7,\n",
      "              enable_categorical=False, eta=0.060000000000000005, gamma=3,\n",
      "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0599999987, max_delta_step=0, max_depth=7,\n",
      "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              predictor='auto', random_state=101, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=4.045069258, seed=101, subsample=0.7,\n",
      "              tree_method='exact', use_label_encoder=False, ...)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6442492343681221\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'subsample': 0.7, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 4, 'max_depth': 7, 'gamma': 3, 'eta': 0.060000000000000005, 'colsample_bytree:': 0.7}\n"
     ]
    }
   ],
   "source": [
    "#STM selected\n",
    "Randomized_search_XGB_STM_selected = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_XGB_STM_selected.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_XGB_STM_selected.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_XGB_STM_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:57:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:57:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6471801925722145"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STM selected final\n",
    "XGB_final_STM_selected = Randomized_search_XGB_STM_selected.best_estimator_\n",
    "XGB_final_STM_selected.fit(STM_X_selected_train, STM_y_selected_train)\n",
    "STM_y_selected_pred_XGB = XGB_final_STM_selected.predict(STM_X_selected_test)\n",
    "#test score\n",
    "f1_score(STM_y_selected_test, STM_y_selected_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[12:01:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:01:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.6,\n",
      "              enable_categorical=False, eta=0.11, gamma=2, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.109999999, max_delta_step=0, max_depth=5,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='binary:logitraw', predictor='auto', random_state=101,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=4.045069258, seed=101,\n",
      "              subsample=0.5, tree_method='exact', ...)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.4737451985266346\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'subsample': 0.5, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logitraw', 'min_child_weight': 3, 'max_depth': 5, 'gamma': 2, 'eta': 0.11, 'colsample_bytree:': 0.6}\n"
     ]
    }
   ],
   "source": [
    "#BERTopic\n",
    "Randomized_search_XGB_Bert = RandomizedSearchCV(XGB, parameters_XGB, verbose=2, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_XGB_Bert.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_XGB_Bert.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_XGB_Bert.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:01:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:01:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4902143522833178"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT final\n",
    "XGB_final_Bert = Randomized_search_XGB_Bert.best_estimator_\n",
    "XGB_final_Bert.fit(Bert_X_train, Bert_y_train)\n",
    "Bert_y_pred_XGB = XGB_final_Bert.predict(Bert_X_test)\n",
    "#test score\n",
    "f1_score(Bert_y_test, Bert_y_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " Results from Randomized Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, colsample_bytree:=0.5,\n",
      "              enable_categorical=False, eta=0.11, gamma=2, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.109999999, max_delta_step=0, max_depth=5,\n",
      "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              predictor='auto', random_state=101, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=4.045069258, seed=101, subsample=0.6,\n",
      "              tree_method='exact', use_label_encoder=False, ...)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.48162299309419543\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'subsample': 0.6, 'scale_pos_weight': 4.045069258, 'objective': 'binary:logistic', 'min_child_weight': 4, 'max_depth': 5, 'gamma': 2, 'eta': 0.11, 'colsample_bytree:': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#Bertopic selected\n",
    "Randomized_search_XGB_Bert_selected = RandomizedSearchCV(XGB, parameters_XGB, verbose=1, scoring=\"f1\", n_jobs = 2)\n",
    "Randomized_search_XGB_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "print(\" Results from Randomized Search \" )\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\",Randomized_search_XGB_Bert_selected.best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\",Randomized_search_XGB_Bert_selected.best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\",Randomized_search_XGB_Bert_selected.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bytree:\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4884510869565217"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bertopic selected final\n",
    "XGB_final_Bert_selected = Randomized_search_XGB_Bert_selected.best_estimator_\n",
    "XGB_final_Bert_selected.fit(Bert_X_selected_train, Bert_y_selected_train)\n",
    "Bert_y_selected_pred_XGB = XGB_final_Bert_selected.predict(Bert_X_selected_test)\n",
    "#test score\n",
    "f1_score(Bert_y_selected_test, Bert_y_selected_pred_XGB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
